{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai pandas numpy rich"
      ],
      "metadata": {
        "id": "HTwi-GXya_JG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99c46d5-5852-4860-901b-4c26a05ea7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (13.9.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich) (2.19.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enrenG5YMMqR"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata, files\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from typing import Dict, List, Any"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "if GEMINI_API_KEY is None:\n",
        "    raise ValueError(\"‚ùå ERROR: Add GEMINI_API_KEY in Colab ‚Üí Secrets\")\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# IMPORTANT: Only use this model throughout\n",
        "MODEL_NAME = \"gemini-2.5-flash-lite\"\n",
        "\n",
        "print(\"‚úÖ Gemini API Connected Successfully\")\n",
        "print(f\"üìå Using Model: {MODEL_NAME}\")\n"
      ],
      "metadata": {
        "id": "ewhxnZssMQ7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc2bf24-1012-44b0-c6ec-18c1af10adf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Connected Successfully\n",
            "üìå Using Model: gemini-2.5-flash-lite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def utc_now():\n",
        "    \"\"\"Get current UTC timestamp\"\"\"\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def safe_api_call(model, prompt, max_retries=3):\n",
        "    \"\"\"Make API call with retry logic\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e) or \"quota\" in str(e).lower():\n",
        "                wait_time = (attempt + 1) * 2\n",
        "                print(f\"‚è≥ Rate limit hit. Waiting {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise e\n",
        "    raise Exception(\"Max retries exceeded\")\n",
        "\n",
        "def clean_json_response(text):\n",
        "    \"\"\"Extract and clean JSON from AI response\"\"\"\n",
        "    text = text.strip()\n",
        "    # Remove markdown code blocks\n",
        "    text = text.replace('```json', '').replace('```', '')\n",
        "    # Find JSON object boundaries\n",
        "    start = text.find('{')\n",
        "    end = text.rfind('}') + 1\n",
        "    if start != -1 and end > start:\n",
        "        text = text[start:end]\n",
        "    return json.loads(text)"
      ],
      "metadata": {
        "id": "Rvrq3cMHMSTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìÇ Upload your CSV file:\")\n",
        "uploaded = files.upload()\n",
        "csv_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(csv_name)\n",
        "\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded: {csv_name}\")\n",
        "print(f\"üìä Shape: {df.shape}\")\n",
        "print(f\"üìã Columns: {list(df.columns)}\")\n",
        "print(\"\\nüîç First 3 rows:\")\n",
        "print(df.head(3))"
      ],
      "metadata": {
        "id": "q-Ks6S6-MSEq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "47d22c4c-0581-40cd-a9b5-3e3c7d646f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Upload your CSV file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f11ff9e1-259b-4992-b447-e3f37a84d412\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f11ff9e1-259b-4992-b447-e3f37a84d412\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving shipping_data_0.csv to shipping_data_0.csv\n",
            "\n",
            "‚úÖ Loaded: shipping_data_0.csv\n",
            "üìä Shape: (110, 6)\n",
            "üìã Columns: ['origin_warehouse', 'destination_store', 'product', 'on_time', 'product_quantity', 'driver_identifier']\n",
            "\n",
            "üîç First 3 rows:\n",
            "                       origin_warehouse                     destination_store  \\\n",
            "0  d5566b15-b071-4acf-8e8e-c98433083b2d  50d33715-4c77-4dd9-8b9d-ff1ca372a2a2   \n",
            "1  c42f0de8-b4f0-4167-abd1-ae79e5e18eea  172eb8f3-1033-4fb6-b66b-d0df09df3161   \n",
            "2  b145f396-de9b-42f1-9cc9-f5b52c3a941c  65e4544d-42ae-4751-9580-bdcb90e5fcda   \n",
            "\n",
            "   product  on_time  product_quantity                     driver_identifier  \n",
            "0   lotion     True                59  d8da0460-cf39-4f38-9fff-6c9b4e344d8a  \n",
            "1  windows     True                28  293ccaec-6592-4f04-aae5-3e238fe62614  \n",
            "2     skis     True                63  80988f09-91a3-4e1b-8e69-13551c53f318  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Dataset Intelligence Agent\n",
        "class DatasetIntelligenceAgent:\n",
        "    \"\"\"Understands what kind of data we're analyzing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.analysis = None\n",
        "\n",
        "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Deep analysis of dataset structure and business context\"\"\"\n",
        "\n",
        "        # Prepare dataset summary\n",
        "        summary = {\n",
        "            \"columns\": list(df.columns),\n",
        "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
        "            \"row_count\": len(df),\n",
        "            \"sample_data\": df.head(5).to_dict('records'),\n",
        "            \"null_counts\": df.isnull().sum().to_dict(),\n",
        "            \"unique_counts\": {col: df[col].nunique() for col in df.columns}\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a business dataset. Determine what domain this belongs to and how to analyze it.\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- Columns: {summary['columns']}\n",
        "- Data types: {summary['dtypes']}\n",
        "- Row count: {summary['row_count']}\n",
        "- Unique value counts: {summary['unique_counts']}\n",
        "- Sample rows: {json.dumps(summary['sample_data'][:3], indent=2, default=str)}\n",
        "\n",
        "TASK: Analyze this dataset and provide a structured understanding.\n",
        "\n",
        "Return ONLY a JSON object with this structure:\n",
        "{{\n",
        "  \"domain\": \"string (e.g., 'supply_chain', 'sales', 'inventory', 'logistics', 'retail')\",\n",
        "  \"dataset_type\": \"string (e.g., 'shipment_tracking', 'order_fulfillment', 'warehouse_inventory')\",\n",
        "  \"business_context\": \"string (brief description of what this data represents)\",\n",
        "  \"key_entities\": [\"list\", \"of\", \"main\", \"entities\"],\n",
        "  \"problem_indicators\": {{\n",
        "    \"column_name\": \"what problem it indicates\"\n",
        "  }},\n",
        "  \"success_metrics\": {{\n",
        "    \"column_name\": \"what success it measures\"\n",
        "  }},\n",
        "  \"potential_issues\": [\"list\", \"of\", \"issues\", \"to\", \"monitor\"],\n",
        "  \"autonomous_actions\": [\"list\", \"of\", \"actions\", \"an\", \"agent\", \"could\", \"take\"],\n",
        "  \"analysis_strategy\": \"how to approach analyzing this data\"\n",
        "}}\n",
        "\n",
        "Be specific based on actual column names and data patterns.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        self.analysis = clean_json_response(response_text)\n",
        "\n",
        "        print(\"\\nü§ñ AI Dataset Analysis:\")\n",
        "        print(json.dumps(self.analysis, indent=2))\n",
        "\n",
        "        return self.analysis\n",
        "\n",
        "# Initialize and run dataset analysis\n",
        "intelligence_agent = DatasetIntelligenceAgent()\n",
        "dataset_context = intelligence_agent.analyze_dataset(df)\n"
      ],
      "metadata": {
        "id": "PiTJJHJ0Mc_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "b6d92e3f-0769-486d-f287-7afe1850fa4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ AI Dataset Analysis:\n",
            "{\n",
            "  \"domain\": \"logistics\",\n",
            "  \"dataset_type\": \"shipment_delivery_performance\",\n",
            "  \"business_context\": \"This dataset tracks shipments from origin warehouses to destination stores, including information about the product, quantity, delivery timeliness, and the driver involved. It appears to be focused on the efficiency and performance of delivery operations.\",\n",
            "  \"key_entities\": [\n",
            "    \"warehouse\",\n",
            "    \"store\",\n",
            "    \"product\",\n",
            "    \"driver\",\n",
            "    \"shipment\"\n",
            "  ],\n",
            "  \"problem_indicators\": {\n",
            "    \"on_time\": \"Indicates missed delivery deadlines. A high number of 'false' values suggests delivery delays, impacting customer satisfaction and operational efficiency.\",\n",
            "    \"origin_warehouse\": \"High cardinality (110 unique values for 110 rows) suggests each row might represent a unique shipment or a very granular tracking of individual shipments. If warehouses are truly unique per row, it might indicate issues with tracking aggregation or data entry, or that the dataset is focused on individual shipment events rather than aggregated warehouse data.\",\n",
            "    \"destination_store\": \"Similar to origin_warehouse, high cardinality suggests individual shipment tracking. If a store receives multiple shipments, this high cardinality is unexpected and might point to data issues or a unique identifier per shipment instance rather than a store entity.\",\n",
            "    \"driver_identifier\": \"High cardinality (110 unique values for 110 rows) suggests each row is linked to a unique driver for that specific shipment. This is highly unusual if drivers handle multiple deliveries. It might indicate each row is a unique assignment, or again, potential data granularity/entry issues. If it's truly unique per row, it limits analysis of driver performance trends.\"\n",
            "  },\n",
            "  \"success_metrics\": {\n",
            "    \"on_time\": \"The primary success metric here is the proportion of deliveries that are 'on_time' (true). High on-time delivery rates are indicative of efficient logistics.\",\n",
            "    \"product_quantity\": \"While not a direct success metric for delivery itself, it's a key operational metric. Understanding the quantity delivered helps in analyzing the load and efficiency of each shipment.\"\n",
            "  },\n",
            "  \"potential_issues\": [\n",
            "    \"Data sparsity: With only 110 rows, detailed trend analysis or identifying statistically significant patterns might be challenging.\",\n",
            "    \"High cardinality in identifiers: The unique values for 'origin_warehouse', 'destination_store', and 'driver_identifier' matching the row count (110) is a significant anomaly. It implies each row represents a unique instance of these entities, which is highly unlikely for warehouses, stores, and drivers in a real-world scenario. This suggests potential data generation issues, each row being a unique shipment event with a unique identifier for each of these fields, or incorrect data aggregation.\",\n",
            "    \"Lack of temporal information: No timestamps are provided, making it impossible to analyze delivery times, transit times, or day-of-week/month-of-year patterns.\",\n",
            "    \"Limited product detail: 'product' is a string, but we don't know if it's a generic name or a specific SKU. More detail could allow for product-specific delivery analysis.\",\n",
            "    \"No cost or route information: Analysis of efficiency is limited without cost data or route details.\"\n",
            "  ],\n",
            "  \"autonomous_actions\": [\n",
            "    \"Monitor 'on_time' delivery rate and flag shipments that are not on time.\",\n",
            "    \"Identify products with consistently low on-time delivery rates.\",\n",
            "    \"If driver identifiers are indeed unique per shipment, flag this as a data anomaly for investigation.\",\n",
            "    \"If the high cardinality is confirmed to be data issue, attempt to group shipments by actual warehouses, stores, and drivers to analyze performance trends.\",\n",
            "    \"Alert when the number of late deliveries for a specific product or destination store exceeds a predefined threshold.\"\n",
            "  ],\n",
            "  \"analysis_strategy\": \"Given the peculiar high cardinality of identifier columns, the first step is to investigate data integrity. Assuming the identifiers are meant to represent actual entities:\\n\\n1.  **Data Cleaning and Validation**: Investigate the high cardinality of 'origin_warehouse', 'destination_store', and 'driver_identifier'. If these are meant to be recurring entities, the current data structure with unique IDs per row is problematic. A crucial step would be to determine if these are truly unique or if there's a way to aggregate them (e.g., if these are unique shipment IDs and the original warehouse/store/driver are attributes of that shipment).\\n\\n2.  **Descriptive Statistics**: Calculate the overall on-time delivery rate. Analyze the distribution of 'product_quantity'.\\n\\n3.  **Performance Analysis (assuming aggregation is possible)**:\\n    *   **On-Time Performance by Product**: Calculate on-time delivery rates for each 'product'. Identify top and bottom performing products.\\n    *   **On-Time Performance by Driver**: If drivers can be aggregated (i.e., a driver handles multiple shipments), analyze their on-time delivery performance.\\n    *   **On-Time Performance by Warehouse/Store**: If warehouses and stores can be aggregated, analyze delivery performance from specific warehouses to specific stores.\\n\\n4.  **Anomaly Detection**: Look for patterns that deviate significantly from the norm, especially concerning late deliveries or unusually large/small product quantities.\\n\\n5.  **Hypothesis Generation**: Based on preliminary findings, hypothesize potential causes for delays (e.g., specific products, specific drivers, certain routes if route data were available).\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Problem Detection Agent\n",
        "class ProblemDetectionAgent:\n",
        "    \"\"\"Scans data for issues requiring intervention\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.problems_found = []\n",
        "\n",
        "    def scan_dataset(self, df: pd.DataFrame) -> List[Dict]:\n",
        "        \"\"\"Analyze entire dataset for patterns and problems\"\"\"\n",
        "\n",
        "        # Create aggregated view for pattern detection\n",
        "        analysis_data = {\n",
        "            \"total_rows\": len(df),\n",
        "            \"problem_indicators\": self.context.get(\"problem_indicators\", {}),\n",
        "            \"column_stats\": {}\n",
        "        }\n",
        "\n",
        "        # Calculate relevant statistics\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"mean\": float(df[col].mean()),\n",
        "                    \"std\": float(df[col].std()),\n",
        "                    \"min\": float(df[col].min()),\n",
        "                    \"max\": float(df[col].max())\n",
        "                }\n",
        "            elif df[col].dtype == 'bool':\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"true_count\": int(df[col].sum()),\n",
        "                    \"false_count\": int((~df[col]).sum()),\n",
        "                    \"true_percentage\": float(df[col].mean() * 100)\n",
        "                }\n",
        "            else:\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"unique_count\": int(df[col].nunique()),\n",
        "                    \"top_values\": df[col].value_counts().head(5).to_dict()\n",
        "                }\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a {self.context['dataset_type']} dataset to find problems.\n",
        "\n",
        "DATASET CONTEXT:\n",
        "{json.dumps(self.context, indent=2)}\n",
        "\n",
        "STATISTICAL ANALYSIS:\n",
        "{json.dumps(analysis_data, indent=2, default=str)}\n",
        "\n",
        "SAMPLE RECORDS:\n",
        "{json.dumps(df.head(10).to_dict('records'), indent=2, default=str)}\n",
        "\n",
        "TASK: Identify specific, actionable problems that require autonomous intervention.\n",
        "\n",
        "Return ONLY a JSON object:\n",
        "{{\n",
        "  \"problems\": [\n",
        "    {{\n",
        "      \"problem_id\": \"PROB-001\",\n",
        "      \"severity\": \"CRITICAL|HIGH|MEDIUM|LOW\",\n",
        "      \"category\": \"category name\",\n",
        "      \"description\": \"specific problem description\",\n",
        "      \"affected_records\": \"how many/which records\",\n",
        "      \"business_impact\": \"what's the business impact\",\n",
        "      \"requires_action\": true/false,\n",
        "      \"suggested_actions\": [\"action1\", \"action2\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"summary\": {{\n",
        "    \"total_problems\": 0,\n",
        "    \"critical_count\": 0,\n",
        "    \"high_count\": 0,\n",
        "    \"requires_immediate_action\": true/false\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Focus on REAL problems in the data, not hypothetical ones.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        result = clean_json_response(response_text)\n",
        "\n",
        "        self.problems_found = result.get(\"problems\", [])\n",
        "\n",
        "        print(f\"\\n‚ö†Ô∏è  PROBLEM SCAN COMPLETE\")\n",
        "        print(f\"   Total Problems Found: {result['summary']['total_problems']}\")\n",
        "        print(f\"   Critical: {result['summary']['critical_count']}\")\n",
        "        print(f\"   High: {result['summary']['high_count']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# Run problem detection\n",
        "problem_detector = ProblemDetectionAgent(dataset_context)\n",
        "problem_analysis = problem_detector.scan_dataset(df)\n"
      ],
      "metadata": {
        "id": "PgFB90joMfEj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d147e4ec-6390-4a73-8a39-5ce630da0406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö†Ô∏è  PROBLEM SCAN COMPLETE\n",
            "   Total Problems Found: 3\n",
            "   Critical: 0\n",
            "   High: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Autonomous Planning Agent\n",
        "class AutonomousPlannerAgent:\n",
        "    \"\"\"Creates detailed action plans for each problem\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.plans = []\n",
        "\n",
        "    def create_plan(self, problem: Dict, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate detailed autonomous action plan\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"You are an autonomous planning agent for {self.context['dataset_type']}.\n",
        "\n",
        "PROBLEM TO SOLVE:\n",
        "{json.dumps(problem, indent=2)}\n",
        "\n",
        "BUSINESS CONTEXT:\n",
        "- Domain: {self.context['domain']}\n",
        "- Key entities: {self.context['key_entities']}\n",
        "- Available actions: {self.context['autonomous_actions']}\n",
        "\n",
        "TASK: Create a detailed, step-by-step execution plan.\n",
        "\n",
        "Return ONLY a JSON object:\n",
        "{{\n",
        "  \"plan_id\": \"PLAN-{problem['problem_id']}\",\n",
        "  \"problem_reference\": \"{problem['problem_id']}\",\n",
        "  \"objective\": \"clear statement of what we're trying to achieve\",\n",
        "  \"execution_steps\": [\n",
        "    {{\n",
        "      \"step_number\": 1,\n",
        "      \"action_type\": \"API_CALL|DATABASE_QUERY|NOTIFICATION|REROUTE|REORDER\",\n",
        "      \"description\": \"what to do\",\n",
        "      \"tool_needed\": \"specific tool/API\",\n",
        "      \"parameters\": {{}},\n",
        "      \"expected_outcome\": \"what success looks like\",\n",
        "      \"rollback_plan\": \"what to do if this fails\"\n",
        "    }}\n",
        "  ],\n",
        "  \"success_criteria\": [\"criterion1\", \"criterion2\"],\n",
        "  \"estimated_impact\": \"quantifiable benefit\",\n",
        "  \"estimated_time\": \"how long to execute\",\n",
        "  \"dependencies\": [\"what needs to happen first\"]\n",
        "}}\n",
        "\n",
        "Make the plan realistic and executable with specific details.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        plan = clean_json_response(response_text)\n",
        "\n",
        "        plan[\"created_at\"] = utc_now()\n",
        "        self.plans.append(plan)\n",
        "\n",
        "        return plan\n",
        "\n",
        "# Create plans for each problem\n",
        "planner_agent = AutonomousPlannerAgent(dataset_context)\n",
        "execution_plans = []\n",
        "\n",
        "print(\"\\nüìã CREATING AUTONOMOUS PLANS...\")\n",
        "for i, problem in enumerate(problem_detector.problems_found):\n",
        "    if problem.get(\"requires_action\"):\n",
        "        print(f\"\\n[{i+1}] Planning for: {problem['problem_id']}\")\n",
        "        plan = planner_agent.create_plan(problem, df)\n",
        "        execution_plans.append(plan)\n",
        "        print(f\"   ‚úì Plan created: {len(plan['execution_steps'])} steps\")\n",
        "        time.sleep(1)  # Rate limiting\n"
      ],
      "metadata": {
        "id": "CY-ew2rqMSBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5d03a003-cf82-4a04-e507-f826e4e8e0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìã CREATING AUTONOMOUS PLANS...\n",
            "\n",
            "[1] Planning for: PROB-001\n",
            "   ‚úì Plan created: 6 steps\n",
            "\n",
            "[2] Planning for: PROB-002\n",
            "   ‚úì Plan created: 5 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Autonomous Execution Agent\n",
        "class ExecutionAgent:\n",
        "    \"\"\"Simulates execution of autonomous plans\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.execution_logs = []\n",
        "\n",
        "    def execute_plan(self, plan: Dict) -> Dict:\n",
        "        \"\"\"Execute each step of the plan\"\"\"\n",
        "\n",
        "        print(f\"\\nüîß EXECUTING: {plan['plan_id']}\")\n",
        "        print(f\"   Objective: {plan['objective']}\")\n",
        "\n",
        "        execution_log = {\n",
        "            \"plan_id\": plan['plan_id'],\n",
        "            \"started_at\": utc_now(),\n",
        "            \"steps_executed\": [],\n",
        "            \"status\": \"IN_PROGRESS\"\n",
        "        }\n",
        "\n",
        "        for step in plan['execution_steps']:\n",
        "            print(f\"\\n   Step {step['step_number']}: {step['description']}\")\n",
        "\n",
        "            # Simulate execution with AI\n",
        "            prompt = f\"\"\"Simulate executing this autonomous action:\n",
        "\n",
        "STEP: {json.dumps(step, indent=2)}\n",
        "\n",
        "CONTEXT: {self.context['dataset_type']} system\n",
        "\n",
        "Simulate realistic execution and return result in JSON:\n",
        "{{\n",
        "  \"step_number\": {step['step_number']},\n",
        "  \"status\": \"SUCCESS|FAILED|PARTIAL\",\n",
        "  \"action_taken\": \"specific action performed\",\n",
        "  \"result_data\": {{}},\n",
        "  \"metrics\": {{}},\n",
        "  \"notes\": \"important details\",\n",
        "  \"timestamp\": \"{utc_now()}\"\n",
        "}}\n",
        "\n",
        "Be realistic about what would actually happen.\"\"\"\n",
        "\n",
        "            response_text = safe_api_call(self.model, prompt)\n",
        "            result = clean_json_response(response_text)\n",
        "\n",
        "            execution_log[\"steps_executed\"].append(result)\n",
        "\n",
        "            status_icon = \"‚úì\" if result[\"status\"] == \"SUCCESS\" else \"‚úó\"\n",
        "            print(f\"      {status_icon} {result['status']}: {result['action_taken']}\")\n",
        "\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "        execution_log[\"completed_at\"] = utc_now()\n",
        "        execution_log[\"status\"] = \"COMPLETED\" if all(\n",
        "            s[\"status\"] == \"SUCCESS\" for s in execution_log[\"steps_executed\"]\n",
        "        ) else \"PARTIAL\"\n",
        "\n",
        "        self.execution_logs.append(execution_log)\n",
        "\n",
        "        print(f\"\\n   ‚úÖ Plan {execution_log['status']}\")\n",
        "\n",
        "        return execution_log\n",
        "\n",
        "# Execute all plans\n",
        "executor_agent = ExecutionAgent(dataset_context)\n",
        "execution_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ AUTONOMOUS EXECUTION STARTED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for plan in execution_plans:\n",
        "    result = executor_agent.execute_plan(plan)\n",
        "    execution_results.append(result)\n",
        "    time.sleep(2)  # Rate limiting between plans\n"
      ],
      "metadata": {
        "id": "BtAMyXKtPFz8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "af804f38-daea-47a6-8803-6c5a6165bed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ AUTONOMOUS EXECUTION STARTED\n",
            "============================================================\n",
            "\n",
            "üîß EXECUTING: PLAN-PROB-001\n",
            "   Objective: Resolve the data integrity issue where 'origin_warehouse', 'destination_store', and 'driver_identifier' columns have unique values per shipment, preventing meaningful performance analysis.\n",
            "\n",
            "   Step 1: Initiate an investigation into the data generation process for shipment data to understand why 'origin_warehouse', 'destination_store', and 'driver_identifier' are unique per row.\n",
            "      ‚úì SUCCESS: Called data_investigation_api with parameters: problem_id='PROB-001', investigation_focus=['data_source_origin', 'etl_processes', 'data_entry_procedures'], columns_of_concern=['origin_warehouse', 'destination_store', 'driver_identifier'].\n",
            "\n",
            "   Step 2: Alert the data engineering and logistics operations teams about the data anomaly and the ongoing investigation.\n",
            "      ‚úì SUCCESS: Sent high-severity alert via alerting_system to data_engineering_lead and logistics_operations_manager.\n",
            "\n",
            "   Step 3: Based on the findings from the investigation, attempt to re-engineer the data to correctly map shipments to actual recurring warehouses, stores, and drivers. This action is conditional on the feasibility of identifying actual entities.\n",
            "      ‚úì SUCCESS: Called data_remediation_api with parameters: {'problem_id': 'PROB-001', 'remediation_strategy': 're_map_entities', 'columns_to_map': {'origin_warehouse': 'actual_warehouse_id', 'destination_store': 'actual_store_id', 'driver_identifier': 'actual_driver_id'}, 'mapping_source': 'entity_master_data'}\n",
            "\n",
            "   Step 4: If re-engineering the data to correctly map to actual entities is not feasible, update the metadata to clearly label these columns as unique shipment identifiers or event IDs.\n",
            "      ‚úì SUCCESS: Called metadata_management_api.update_column_description with parameters: {\"problem_id\": \"PROB-001\", \"action\": \"update_column_description\", \"columns\": [\"origin_warehouse\", \"destination_store\", \"driver_identifier\"], \"new_description\": \"This column represents a unique identifier for the shipment event, not a distinct entity like a warehouse, store, or driver.\"}\n",
            "\n",
            "   Step 5: Trigger a re-evaluation of performance analysis configurations and reports to incorporate the corrected or re-labeled data.\n",
            "      ‚úì SUCCESS: Called performance_analysis_api.reconfigure_analysis with problem_id='PROB-001' and affected_columns=['origin_warehouse', 'destination_store', 'driver_identifier'].\n",
            "\n",
            "   Step 6: Notify stakeholders that the data integrity issue has been addressed and that performance analysis can now proceed with accurate data.\n",
            "      ‚úì SUCCESS: Sent notification via alerting_system.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n",
            "\n",
            "üîß EXECUTING: PLAN-PROB-002\n",
            "   Objective: To significantly improve the on-time delivery rate from the current 44.55% by identifying and addressing the root causes of delivery delays.\n",
            "\n",
            "   Step 1: Continuously monitor the 'on_time' delivery rate for all shipments. This involves fetching real-time delivery status data. Flag any shipment that deviates from its expected delivery schedule.\n",
            "      ‚úì SUCCESS: Initiated a continuous stream to monitor shipments with delivery status other than 'on_time' using the ShipmentTrackingAPI.\n",
            "\n",
            "   Step 2: Analyze historical shipment data to identify products with consistently low on-time delivery rates. This requires querying the shipment database for on-time performance aggregated by product.\n",
            "      ‚úì SUCCESS: Executed SQL query against the shipment database.\n",
            "\n",
            "   Step 3: Alert relevant stakeholders (e.g., logistics managers, product managers) when the number of late deliveries for a specific product or destination store exceeds a predefined threshold. This action is dependent on the findings from step 2 and potentially from PROB-001 (if data quality improves).\n",
            "      ‚úì SUCCESS: Sent high severity alert via AlertingSystem.\n",
            "\n",
            "   Step 4: Assuming data quality for driver identifiers improves (as suggested by PROB-001 potentially), flag shipments where driver identifiers appear inconsistent or of high cardinality, indicating potential data anomaly. This step is preparatory for deeper analysis.\n",
            "      ‚úì SUCCESS: Executed SQL query: SELECT driver_id, COUNT(DISTINCT shipment_id) as shipments FROM shipments GROUP BY driver_id HAVING COUNT(DISTINCT shipment_id) > 1000 OR COUNT(DISTINCT driver_id) > 5000;\n",
            "\n",
            "   Step 5: If driver identifiers are confirmed to be unique and valid per shipment, analyze performance by driver. This involves grouping shipments by driver and calculating on-time delivery rates for each. This step is contingent on the outcome of Step 4 and PROB-001.\n",
            "      ‚úì SUCCESS: Executed database query to analyze driver performance.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Generate Final Report\n",
        "final_report = {\n",
        "    \"analysis_metadata\": {\n",
        "        \"dataset_name\": csv_name,\n",
        "        \"analyzed_at\": utc_now(),\n",
        "        \"total_rows\": len(df),\n",
        "        \"model_used\": MODEL_NAME\n",
        "    },\n",
        "    \"dataset_intelligence\": dataset_context,\n",
        "    \"problem_detection\": problem_analysis,\n",
        "    \"autonomous_plans\": execution_plans,\n",
        "    \"execution_results\": execution_results,\n",
        "    \"summary\": {\n",
        "        \"problems_detected\": len(problem_detector.problems_found),\n",
        "        \"plans_created\": len(execution_plans),\n",
        "        \"plans_executed\": len(execution_results),\n",
        "        \"successful_executions\": sum(1 for r in execution_results if r[\"status\"] == \"COMPLETED\"),\n",
        "        \"total_steps_executed\": sum(len(r[\"steps_executed\"]) for r in execution_results)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_filename = f\"autonomous_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "\n",
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(final_report, f, indent=2, default=str)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Analysis Complete!\")\n",
        "print(f\"üìÅ Output File: {output_filename}\")\n",
        "print(f\"\\nüìà Summary:\")\n",
        "print(f\"   ‚Ä¢ Problems Detected: {final_report['summary']['problems_detected']}\")\n",
        "print(f\"   ‚Ä¢ Autonomous Plans: {final_report['summary']['plans_created']}\")\n",
        "print(f\"   ‚Ä¢ Successfully Executed: {final_report['summary']['successful_executions']}\")\n",
        "print(f\"   ‚Ä¢ Total Steps: {final_report['summary']['total_steps_executed']}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Download the file\n",
        "files.download(output_filename)\n",
        "\n",
        "print(\"\\n‚ú® All done! Check your downloads folder for the JSON report.\")\n"
      ],
      "metadata": {
        "id": "XVpKxl1cMR92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "14face37-1c1b-4037-be73-8fb4939b9ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä FINAL REPORT\n",
            "============================================================\n",
            "‚úÖ Analysis Complete!\n",
            "üìÅ Output File: autonomous_analysis_20251129_151841.json\n",
            "\n",
            "üìà Summary:\n",
            "   ‚Ä¢ Problems Detected: 3\n",
            "   ‚Ä¢ Autonomous Plans: 2\n",
            "   ‚Ä¢ Successfully Executed: 2\n",
            "   ‚Ä¢ Total Steps: 11\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e5400db6-765d-478a-af90-84e79f9c8362\", \"autonomous_analysis_20251129_151841.json\", 42429)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ú® All done! Check your downloads folder for the JSON report.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac492fa7",
        "outputId": "118e2544-d588-4172-cd9c-5c003004db8c"
      },
      "source": [
        "class DataPreprocessingAgent:\n",
        "    \"\"\"Responsible for identifying and handling missing values, correcting data types, and detecting outliers.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.copy()\n",
        "        self.missing_values_report = {}\n",
        "        self.datatype_corrections = {}\n",
        "        self.outlier_report = {}\n",
        "\n",
        "    def clean_missing_values(self) -> pd.DataFrame:\n",
        "        \"\"\"Identifies and handles missing values. Fills numerical NaNs with median, categorical with mode.\"\"\"\n",
        "        print(\"\\nüîç Cleaning missing values...\")\n",
        "        for col in self.df.columns:\n",
        "            if self.df[col].isnull().any():\n",
        "                missing_count = self.df[col].isnull().sum()\n",
        "                missing_percentage = (missing_count / len(self.df)) * 100\n",
        "                self.missing_values_report[col] = {\n",
        "                    \"count\": missing_count,\n",
        "                    \"percentage\": f\"{missing_percentage:.2f}%\"\n",
        "                }\n",
        "\n",
        "                if self.df[col].dtype in ['int64', 'float64']:\n",
        "                    median_val = self.df[col].median()\n",
        "                    self.df[col].fillna(median_val, inplace=True)\n",
        "                    print(f\"   - Column '{col}': Filled {missing_count} missing numerical values with median ({median_val}).\")\n",
        "                elif self.df[col].dtype == 'object' or self.df[col].dtype == 'bool': # Also cover boolean in case of NaN\n",
        "                    mode_val = self.df[col].mode()[0] if not self.df[col].mode().empty else None\n",
        "                    if mode_val is not None:\n",
        "                        self.df[col].fillna(mode_val, inplace=True)\n",
        "                        print(f\"   - Column '{col}': Filled {missing_count} missing categorical/boolean values with mode ({mode_val}).\")\n",
        "                    else:\n",
        "                        print(f\"   - Column '{col}': Could not determine mode for missing values, leaving as is.\")\n",
        "\n",
        "        if not self.missing_values_report:\n",
        "            print(\"   No missing values found.\")\n",
        "        print(\"   Missing value cleaning complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def correct_datatypes(self) -> pd.DataFrame:\n",
        "        \"\"\"Infers and corrects data types for columns.\"\"\"\n",
        "        print(\"\\n‚öôÔ∏è Correcting data types...\")\n",
        "        initial_dtypes = self.df.dtypes.to_dict()\n",
        "        for col in self.df.columns:\n",
        "            # Try to convert object columns to numeric if possible\n",
        "            if self.df[col].dtype == 'object':\n",
        "                try:\n",
        "                    # Attempt to convert to numeric, coercing errors to NaN\n",
        "                    converted = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if not converted.isnull().all(): # If not all values became NaN, it's numeric\n",
        "                        self.df[col] = converted\n",
        "                        if initial_dtypes[col] != self.df[col].dtype:\n",
        "                            self.datatype_corrections[col] = f\"Changed from {initial_dtypes[col]} to {self.df[col].dtype}\"\n",
        "                            print(f\"   - Column '{col}': Converted to numeric.\")\n",
        "                    else:\n",
        "                        # Attempt to convert to datetime\n",
        "                        converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
        "                        if not converted_dt.isnull().all():\n",
        "                            self.df[col] = converted_dt\n",
        "                            if initial_dtypes[col] != self.df[col].dtype:\n",
        "                                self.datatype_corrections[col] = f\"Changed from {initial_dtypes[col]} to {self.df[col].dtype}\"\n",
        "                                print(f\"   - Column '{col}': Converted to datetime.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   - Column '{col}': Could not convert object type. Error: {e}\")\n",
        "\n",
        "        if not self.datatype_corrections:\n",
        "            print(\"   No datatype corrections made.\")\n",
        "        print(\"   Data type correction complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def detect_outliers(self, numerical_cols: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Identifies outliers in numerical columns using the IQR method.\"\"\"\n",
        "        print(\"\\nüìä Detecting outliers...\")\n",
        "        if numerical_cols is None:\n",
        "            numerical_cols = self.df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            if col in self.df.columns:\n",
        "                Q1 = self.df[col].quantile(0.25)\n",
        "                Q3 = self.df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "                outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]\n",
        "                if not outliers.empty:\n",
        "                    self.outlier_report[col] = {\n",
        "                        \"count\": len(outliers),\n",
        "                        \"percentage\": f\"{(len(outliers) / len(self.df)) * 100:.2f}%\",\n",
        "                        \"lower_bound\": lower_bound,\n",
        "                        \"upper_bound\": upper_bound\n",
        "                    }\n",
        "                    print(f\"   - Column '{col}': Detected {len(outliers)} outliers (outside [{lower_bound:.2f}, {upper_bound:.2f}]).\")\n",
        "\n",
        "        if not self.outlier_report:\n",
        "            print(\"   No significant outliers detected in numerical columns.\")\n",
        "        print(\"   Outlier detection complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Orchestrates the execution of preprocessing steps and returns the cleaned DataFrame.\"\"\"\n",
        "        print(\"\\nüöÄ Starting data preprocessing...\")\n",
        "\n",
        "        self.df = self.clean_missing_values()\n",
        "        self.df = self.correct_datatypes()\n",
        "        self.df = self.detect_outliers()\n",
        "\n",
        "        print(\"\\n‚úÖ Data preprocessing complete.\")\n",
        "        print(\"\\n--- Preprocessing Report ---\")\n",
        "        print(\"Missing Values Report:\", self.missing_values_report)\n",
        "        print(\"Datatype Corrections:\", self.datatype_corrections)\n",
        "        print(\"Outlier Report:\", self.outlier_report)\n",
        "        print(\"--------------------------\")\n",
        "        return self.df\n",
        "\n",
        "# Instantiate and run the DataPreprocessingAgent\n",
        "preprocessing_agent = DataPreprocessingAgent(df)\n",
        "preprocessed_df = preprocessing_agent.preprocess_data()\n",
        "\n",
        "print(\"\\nPreprocessed DataFrame head:\")\n",
        "print(preprocessed_df.head())\n",
        "print(\"\\nPreprocessed DataFrame dtypes:\")\n",
        "print(preprocessed_df.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting data preprocessing...\n",
            "\n",
            "üîç Cleaning missing values...\n",
            "   No missing values found.\n",
            "   Missing value cleaning complete.\n",
            "\n",
            "‚öôÔ∏è Correcting data types...\n",
            "   No datatype corrections made.\n",
            "   Data type correction complete.\n",
            "\n",
            "üìä Detecting outliers...\n",
            "   No significant outliers detected in numerical columns.\n",
            "   Outlier detection complete.\n",
            "\n",
            "‚úÖ Data preprocessing complete.\n",
            "\n",
            "--- Preprocessing Report ---\n",
            "Missing Values Report: {}\n",
            "Datatype Corrections: {}\n",
            "Outlier Report: {}\n",
            "--------------------------\n",
            "\n",
            "Preprocessed DataFrame head:\n",
            "                       origin_warehouse                     destination_store  \\\n",
            "0  d5566b15-b071-4acf-8e8e-c98433083b2d  50d33715-4c77-4dd9-8b9d-ff1ca372a2a2   \n",
            "1  c42f0de8-b4f0-4167-abd1-ae79e5e18eea  172eb8f3-1033-4fb6-b66b-d0df09df3161   \n",
            "2  b145f396-de9b-42f1-9cc9-f5b52c3a941c  65e4544d-42ae-4751-9580-bdcb90e5fcda   \n",
            "3  f4372224-759f-43b3-bc83-ca6106bba1af  745bee4e-710c-4538-8df1-5c146e1092a6   \n",
            "4  49d0edae-9091-41bb-a08d-ab1c66bd08d5  425b7a1a-b744-4c6b-898e-d424dd8cf18e   \n",
            "\n",
            "   product  on_time  product_quantity                     driver_identifier  \n",
            "0   lotion     True                59  d8da0460-cf39-4f38-9fff-6c9b4e344d8a  \n",
            "1  windows     True                28  293ccaec-6592-4f04-aae5-3e238fe62614  \n",
            "2     skis     True                63  80988f09-91a3-4e1b-8e69-13551c53f318  \n",
            "3    bikes     True                47  5f79b402-655f-4d8e-8ff3-5ef05870e0ad  \n",
            "4    candy    False                73  58beb5d3-98f8-4077-a964-1f04f7cb11e5  \n",
            "\n",
            "Preprocessed DataFrame dtypes:\n",
            "origin_warehouse     object\n",
            "destination_store    object\n",
            "product              object\n",
            "on_time                bool\n",
            "product_quantity      int64\n",
            "driver_identifier    object\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2466640132.py:56: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
            "/tmp/ipython-input-2466640132.py:56: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
            "/tmp/ipython-input-2466640132.py:56: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
            "/tmp/ipython-input-2466640132.py:56: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1a4709d",
        "outputId": "b23d5d6b-babd-4b54-e937-0be660c3ade4"
      },
      "source": [
        "import pandas as pd\n",
        "from typing import List\n",
        "\n",
        "class DataPreprocessingAgent:\n",
        "    \"\"\"Responsible for identifying and handling missing values, correcting data types, and detecting outliers.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.copy()\n",
        "        self.missing_values_report = {}\n",
        "        self.datatype_corrections = {}\n",
        "        self.outlier_report = {}\n",
        "\n",
        "    def clean_missing_values(self) -> pd.DataFrame:\n",
        "        \"\"\"Identifies and handles missing values. Fills numerical NaNs with median, categorical with mode.\"\"\"\n",
        "        print(\"\\nüîç Cleaning missing values...\")\n",
        "        for col in self.df.columns:\n",
        "            if self.df[col].isnull().any():\n",
        "                missing_count = self.df[col].isnull().sum()\n",
        "                missing_percentage = (missing_count / len(self.df)) * 100\n",
        "                self.missing_values_report[col] = {\n",
        "                    \"count\": missing_count,\n",
        "                    \"percentage\": f\"{missing_percentage:.2f}%\"\n",
        "                }\n",
        "\n",
        "                if self.df[col].dtype in ['int64', 'float64']:\n",
        "                    median_val = self.df[col].median()\n",
        "                    self.df[col].fillna(median_val, inplace=True)\n",
        "                    print(f\"   - Column '{col}': Filled {missing_count} missing numerical values with median ({median_val}).\")\n",
        "                elif self.df[col].dtype == 'object' or self.df[col].dtype == 'bool': # Also cover boolean in case of NaN\n",
        "                    mode_val = self.df[col].mode()[0] if not self.df[col].mode().empty else None\n",
        "                    if mode_val is not None:\n",
        "                        self.df[col].fillna(mode_val, inplace=True)\n",
        "                        print(f\"   - Column '{col}': Filled {missing_count} missing categorical/boolean values with mode ({mode_val}).\")\n",
        "                    else:\n",
        "                        print(f\"   - Column '{col}': Could not determine mode for missing values, leaving as is.\")\n",
        "\n",
        "        if not self.missing_values_report:\n",
        "            print(\"   No missing values found.\")\n",
        "        print(\"   Missing value cleaning complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def correct_datatypes(self) -> pd.DataFrame:\n",
        "        \"\"\"Infers and corrects data types for columns.\"\"\"\n",
        "        print(\"\\n‚öôÔ∏è Correcting data types...\")\n",
        "        initial_dtypes = self.df.dtypes.to_dict()\n",
        "        for col in self.df.columns:\n",
        "            # Try to convert object columns to numeric if possible\n",
        "            if self.df[col].dtype == 'object':\n",
        "                try:\n",
        "                    # Attempt to convert to numeric, coercing errors to NaN\n",
        "                    converted = pd.to_numeric(self.df[col], errors='coerce')\n",
        "                    if not converted.isnull().all(): # If not all values became NaN, it's numeric\n",
        "                        self.df[col] = converted\n",
        "                        if initial_dtypes[col] != self.df[col].dtype:\n",
        "                            self.datatype_corrections[col] = f\"Changed from {initial_dtypes[col]} to {self.df[col].dtype}\"\n",
        "                            print(f\"   - Column '{col}': Converted to numeric.\")\n",
        "                    else:\n",
        "                        # Attempt to convert to datetime\n",
        "                        converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
        "                        if not converted_dt.isnull().all():\n",
        "                            self.df[col] = converted_dt\n",
        "                            if initial_dtypes[col] != self.df[col].dtype:\n",
        "                                self.datatype_corrections[col] = f\"Changed from {initial_dtypes[col]} to {self.df[col].dtype}\"\n",
        "                                print(f\"   - Column '{col}': Converted to datetime.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"   - Column '{col}': Could not convert object type. Error: {e}\")\n",
        "\n",
        "        if not self.datatype_corrections:\n",
        "            print(\"   No datatype corrections made.\")\n",
        "        print(\"   Data type correction complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def detect_outliers(self, numerical_cols: List[str] = None) -> pd.DataFrame:\n",
        "        \"\"\"Identifies outliers in numerical columns using the IQR method.\"\"\"\n",
        "        print(\"\\nüìä Detecting outliers...\")\n",
        "        if numerical_cols is None:\n",
        "            numerical_cols = self.df.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            if col in self.df.columns:\n",
        "                Q1 = self.df[col].quantile(0.25)\n",
        "                Q3 = self.df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "                outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]\n",
        "                if not outliers.empty:\n",
        "                    self.outlier_report[col] = {\n",
        "                        \"count\": len(outliers),\n",
        "                        \"percentage\": f\"{(len(outliers) / len(self.df)) * 100:.2f}%\",\n",
        "                        \"lower_bound\": lower_bound,\n",
        "                        \"upper_bound\": upper_bound\n",
        "                    }\n",
        "                    print(f\"   - Column '{col}': Detected {len(outliers)} outliers (outside [{lower_bound:.2f}, {upper_bound:.2f}]).\")\n",
        "\n",
        "        if not self.outlier_report:\n",
        "            print(\"   No significant outliers detected in numerical columns.\")\n",
        "        print(\"   Outlier detection complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Orchestrates the execution of preprocessing steps and returns the cleaned DataFrame.\"\"\"\n",
        "        print(\"\\nüöÄ Starting data preprocessing...\")\n",
        "\n",
        "        self.df = self.clean_missing_values()\n",
        "        self.df = self.correct_datatypes()\n",
        "        self.df = self.detect_outliers()\n",
        "\n",
        "        print(\"\\n‚úÖ Data preprocessing complete.\")\n",
        "        print(\"\\n--- Preprocessing Report ---\")\n",
        "        print(\"Missing Values Report:\", self.missing_values_report)\n",
        "        print(\"Datatype Corrections:\", self.datatype_corrections)\n",
        "        print(\"Outlier Report:\", self.outlier_report)\n",
        "        print(\"--------------------------\")\n",
        "        return self.df\n",
        "\n",
        "# Instantiate and run the DataPreprocessingAgent\n",
        "preprocessing_agent = DataPreprocessingAgent(df)\n",
        "preprocessed_df = preprocessing_agent.preprocess_data()\n",
        "\n",
        "print(\"\\nPreprocessed DataFrame head:\")\n",
        "print(preprocessed_df.head())\n",
        "print(\"\\nPreprocessed DataFrame dtypes:\")\n",
        "print(preprocessed_df.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting data preprocessing...\n",
            "\n",
            "üîç Cleaning missing values...\n",
            "   No missing values found.\n",
            "   Missing value cleaning complete.\n",
            "\n",
            "‚öôÔ∏è Correcting data types...\n",
            "   No datatype corrections made.\n",
            "   Data type correction complete.\n",
            "\n",
            "üìä Detecting outliers...\n",
            "   No significant outliers detected in numerical columns.\n",
            "   Outlier detection complete.\n",
            "\n",
            "‚úÖ Data preprocessing complete.\n",
            "\n",
            "--- Preprocessing Report ---\n",
            "Missing Values Report: {}\n",
            "Datatype Corrections: {}\n",
            "Outlier Report: {}\n",
            "--------------------------\n",
            "\n",
            "Preprocessed DataFrame head:\n",
            "                       origin_warehouse                     destination_store  \\\n",
            "0  d5566b15-b071-4acf-8e8e-c98433083b2d  50d33715-4c77-4dd9-8b9d-ff1ca372a2a2   \n",
            "1  c42f0de8-b4f0-4167-abd1-ae79e5e18eea  172eb8f3-1033-4fb6-b66b-d0df09df3161   \n",
            "2  b145f396-de9b-42f1-9cc9-f5b52c3a941c  65e4544d-42ae-4751-9580-bdcb90e5fcda   \n",
            "3  f4372224-759f-43b3-bc83-ca6106bba1af  745bee4e-710c-4538-8df1-5c146e1092a6   \n",
            "4  49d0edae-9091-41bb-a08d-ab1c66bd08d5  425b7a1a-b744-4c6b-898e-d424dd8cf18e   \n",
            "\n",
            "   product  on_time  product_quantity                     driver_identifier  \n",
            "0   lotion     True                59  d8da0460-cf39-4f38-9fff-6c9b4e344d8a  \n",
            "1  windows     True                28  293ccaec-6592-4f04-aae5-3e238fe62614  \n",
            "2     skis     True                63  80988f09-91a3-4e1b-8e69-13551c53f318  \n",
            "3    bikes     True                47  5f79b402-655f-4d8e-8ff3-5ef05870e0ad  \n",
            "4    candy    False                73  58beb5d3-98f8-4077-a964-1f04f7cb11e5  \n",
            "\n",
            "Preprocessed DataFrame dtypes:\n",
            "origin_warehouse     object\n",
            "destination_store    object\n",
            "product              object\n",
            "on_time                bool\n",
            "product_quantity      int64\n",
            "driver_identifier    object\n",
            "dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3118946437.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
            "/tmp/ipython-input-3118946437.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
            "/tmp/ipython-input-3118946437.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n",
            "/tmp/ipython-input-3118946437.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  converted_dt = pd.to_datetime(self.df[col], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "36e7c375",
        "outputId": "f78168f6-1168-403a-c09b-5e9483febdcc"
      },
      "source": [
        "class ExecutionAgent:\n",
        "    \"\"\"Simulates execution of autonomous plans\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.execution_logs = []\n",
        "\n",
        "    def execute_plan(self, plan: Dict) -> Dict:\n",
        "        \"\"\"Execute each step of the plan\"\"\"\n",
        "\n",
        "        print(f\"\\nüîß EXECUTING: {plan['plan_id']}\")\n",
        "        print(f\"   Objective: {plan['objective']}\")\n",
        "\n",
        "        execution_log = {\n",
        "            \"plan_id\": plan['plan_id'],\n",
        "            \"started_at\": utc_now(),\n",
        "            \"steps_executed\": [],\n",
        "            \"status\": \"IN_PROGRESS\"\n",
        "        }\n",
        "\n",
        "        for i, step in enumerate(plan['execution_steps']):\n",
        "            step_number = step.get('step_number', i + 1) # Use existing step_number or assign based on index\n",
        "            print(f\"\\n   Step {step_number}: {step['description']}\")\n",
        "\n",
        "            # Simulate execution with AI\n",
        "            prompt = f\"\"\"Simulate executing this autonomous action:\\n\\nSTEP: {json.dumps(step, indent=2)}\\n\\nCONTEXT: {self.context['dataset_type']} system\\n\\nSimulate realistic execution and return result in JSON:\\n{{\\n  \"step_number\": {step_number},\\n  \"status\": \"SUCCESS|FAILED|PARTIAL\",\\n  \"action_taken\": \"specific action performed\",\\n  \"result_data\": {{}},\\n  \"metrics\": {{}},\\n  \"notes\": \"important details\",\\n  \"timestamp\": \"{utc_now()}\"\\n}}\\n\\nBe realistic about what would actually happen.\"\"\"\n",
        "\n",
        "            response_text = safe_api_call(self.model, prompt)\n",
        "            result = clean_json_response(response_text)\n",
        "\n",
        "            execution_log[\"steps_executed\"].append(result)\n",
        "\n",
        "            status_icon = \"‚úì\" if result[\"status\"] == \"SUCCESS\" else \"‚úó\"\n",
        "            print(f\"      {status_icon} {result['status']}: {result['action_taken']}\")\n",
        "\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "        execution_log[\"completed_at\"] = utc_now()\n",
        "        execution_log[\"status\"] = \"COMPLETED\" if all(\n",
        "            s[\"status\"] == \"SUCCESS\" for s in execution_log[\"steps_executed\"]\n",
        "        ) else \"PARTIAL\"\n",
        "\n",
        "        self.execution_logs.append(execution_log)\n",
        "\n",
        "        print(f\"\\n   ‚úÖ Plan {execution_log['status']}\")\n",
        "\n",
        "        return execution_log\n",
        "\n",
        "# Execute all plans\n",
        "executor_agent = ExecutionAgent(dataset_context)\n",
        "execution_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ AUTONOMOUS EXECUTION STARTED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for plan in execution_plans:\n",
        "    result = executor_agent.execute_plan(plan)\n",
        "    execution_results.append(result)\n",
        "    time.sleep(2)  # Rate limiting between plans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üöÄ AUTONOMOUS EXECUTION STARTED\n",
            "============================================================\n",
            "\n",
            "üîß EXECUTING: PLAN-PROB-001\n",
            "   Objective: Resolve the data integrity issue where 'origin_warehouse', 'destination_store', and 'driver_identifier' columns have unique values per shipment, preventing meaningful performance analysis.\n",
            "\n",
            "   Step 1: Initiate an investigation into the data generation process for shipment data to understand why 'origin_warehouse', 'destination_store', and 'driver_identifier' are unique per row.\n",
            "      ‚úì SUCCESS: Called data_investigation_api with parameters: problem_id='PROB-001', investigation_focus=['data_source_origin', 'etl_processes', 'data_entry_procedures'], columns_of_concern=['origin_warehouse', 'destination_store', 'driver_identifier'].\n",
            "\n",
            "   Step 2: Alert the data engineering and logistics operations teams about the data anomaly and the ongoing investigation.\n",
            "      ‚úì SUCCESS: Sent high-severity alert via alerting_system to data_engineering_lead and logistics_operations_manager.\n",
            "\n",
            "   Step 3: Based on the findings from the investigation, attempt to re-engineer the data to correctly map shipments to actual recurring warehouses, stores, and drivers. This action is conditional on the feasibility of identifying actual entities.\n",
            "      ‚úì SUCCESS: Called data_remediation_api with parameters: {\"problem_id\": \"PROB-001\", \"remediation_strategy\": \"re_map_entities\", \"columns_to_map\": {\"origin_warehouse\": \"actual_warehouse_id\", \"destination_store\": \"actual_store_id\", \"driver_identifier\": \"actual_driver_id\"}, \"mapping_source\": \"entity_master_data\"}\n",
            "\n",
            "   Step 4: If re-engineering the data to correctly map to actual entities is not feasible, update the metadata to clearly label these columns as unique shipment identifiers or event IDs.\n",
            "      ‚úì SUCCESS: Called metadata_management_api.update_column_description with parameters: {'problem_id': 'PROB-001', 'columns': ['origin_warehouse', 'destination_store', 'driver_identifier'], 'new_description': 'This column represents a unique identifier for the shipment event, not a distinct entity like a warehouse, store, or driver.'}\n",
            "\n",
            "   Step 5: Trigger a re-evaluation of performance analysis configurations and reports to incorporate the corrected or re-labeled data.\n",
            "      ‚úì SUCCESS: Called performance_analysis_api.reconfigure_analysis with problem_id='PROB-001' and affected_columns=['origin_warehouse', 'destination_store', 'driver_identifier'].\n",
            "\n",
            "   Step 6: Notify stakeholders that the data integrity issue has been addressed and that performance analysis can now proceed with accurate data.\n",
            "      ‚úì SUCCESS: Successfully sent notification via the alerting_system.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n",
            "\n",
            "üîß EXECUTING: PLAN-PROB-002\n",
            "   Objective: To significantly improve the on-time delivery rate from the current 44.55% by identifying and addressing the root causes of delivery delays.\n",
            "\n",
            "   Step 1: Continuously monitor the 'on_time' delivery rate for all shipments. This involves fetching real-time delivery status data. Flag any shipment that deviates from its expected delivery schedule.\n",
            "      ‚úì SUCCESS: Called ShipmentTrackingAPI with filter 'delivery_status != \"on_time\"'.\n",
            "\n",
            "   Step 2: Analyze historical shipment data to identify products with consistently low on-time delivery rates. This requires querying the shipment database for on-time performance aggregated by product.\n",
            "      ‚úì SUCCESS: Executed SQL query on the shipment database.\n",
            "\n",
            "   Step 3: Alert relevant stakeholders (e.g., logistics managers, product managers) when the number of late deliveries for a specific product or destination store exceeds a predefined threshold. This action is dependent on the findings from step 2 and potentially from PROB-001 (if data quality improves).\n",
            "      ‚úì SUCCESS: AlertingSystem.send_alert\n",
            "\n",
            "   Step 4: Assuming data quality for driver identifiers improves (as suggested by PROB-001 potentially), flag shipments where driver identifiers appear inconsistent or of high cardinality, indicating potential data anomaly. This step is preparatory for deeper analysis.\n",
            "      ‚úì SUCCESS: Executed SQL query against the shipments table to identify drivers with an unusually high number of shipments or a very large number of distinct driver IDs (which is a slightly unusual condition in the query itself but simulating for fidelity). The query aimed to find drivers associated with more than 1000 distinct shipments, or more than 5000 distinct driver IDs (this latter part of the `HAVING` clause is likely a typo in the original prompt, as `COUNT(DISTINCT driver_id)` within a `GROUP BY driver_id` context would always be 1 for each group. Assuming it was intended to be something else, or the original intent was to catch cases where a driver appears in a very large number of *different* driver_id records which is also odd, but simulating the literal query).\n",
            "\n",
            "   Step 5: If driver identifiers are confirmed to be unique and valid per shipment, analyze performance by driver. This involves grouping shipments by driver and calculating on-time delivery rates for each. This step is contingent on the outcome of Step 4 and PROB-001.\n",
            "      ‚úì SUCCESS: Executed database query to analyze driver performance.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f74c71b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f120fc6e-b2ce-4047-e185-4353d4cbf682"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "class VisualizationAgent:\n",
        "    \"\"\"Generates interactive plots for key insights from the dataset intelligence and problem detection phases.\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, dataset_context: Dict, problem_analysis: Dict):\n",
        "        self.df = df.copy()\n",
        "        self.dataset_context = dataset_context\n",
        "        self.problem_analysis = problem_analysis\n",
        "        print(\"\\nüìä VisualizationAgent Initialized.\")\n",
        "\n",
        "    def _plot_product_quantity_distribution(self):\n",
        "        \"\"\"Visualizes the distribution of product_quantity using an interactive histogram.\"\"\"\n",
        "        print(\"Generating Product Quantity Distribution plot...\")\n",
        "        fig = px.histogram(self.df, x='product_quantity',\n",
        "                           title='Distribution of Product Quantity',\n",
        "                           labels={'product_quantity': 'Product Quantity'})\n",
        "        fig.show()\n",
        "\n",
        "    def _plot_on_time_rates(self, group_col: str, title_prefix: str):\n",
        "        \"\"\"Visualizes on-time delivery rates broken down by a specified grouping column.\"\"\"\n",
        "        print(f\"Generating On-Time Delivery Rates by {group_col} plot...\")\n",
        "        # Calculate on-time delivery rate\n",
        "        on_time_rates = self.df.groupby(group_col)['on_time'].mean().reset_index()\n",
        "        on_time_rates['on_time_percentage'] = on_time_rates['on_time'] * 100\n",
        "\n",
        "        fig = px.bar(on_time_rates.sort_values(by='on_time_percentage', ascending=False),\n",
        "                     x=group_col, y='on_time_percentage',\n",
        "                     title=f'{title_prefix} On-Time Delivery Percentage',\n",
        "                     labels={'on_time_percentage': 'On-Time Delivery (%)', group_col: group_col.replace('_', ' ').title()},\n",
        "                     color='on_time_percentage', color_continuous_scale=px.colors.sequential.Plasma)\n",
        "        fig.update_layout(xaxis_tickangle=-45)\n",
        "        fig.show()\n",
        "\n",
        "    def _plot_problem_hotspots(self):\n",
        "        \"\"\"Highlights problem hotspots identified by the ProblemDetectionAgent.\"\"\"\n",
        "        print(\"Generating Problem Hotspots visualization...\")\n",
        "        problems = self.problem_analysis.get('problems', [])\n",
        "\n",
        "        # Example: if a problem indicates specific drivers or products are an issue\n",
        "        problem_drivers = set()\n",
        "        problem_products = set()\n",
        "\n",
        "        for problem in problems:\n",
        "            if 'driver' in problem.get('category', '').lower() or 'driver' in problem.get('description', '').lower():\n",
        "                # This is a heuristic, in a real system, problem details would be more structured\n",
        "                if 'driver_identifier' in self.df.columns:\n",
        "                    # Try to extract driver IDs from description if not explicitly listed\n",
        "                    for driver_id in self.df['driver_identifier'].unique():\n",
        "                        if driver_id in problem.get('description', ''):\n",
        "                            problem_drivers.add(driver_id)\n",
        "\n",
        "            if 'product' in problem.get('category', '').lower() or 'product' in problem.get('description', '').lower():\n",
        "                 if 'product' in self.df.columns:\n",
        "                    for prod_name in self.df['product'].unique():\n",
        "                        if prod_name in problem.get('description', ''):\n",
        "                            problem_products.add(prod_name)\n",
        "\n",
        "        if problem_drivers or problem_products:\n",
        "            print(\"   Highlighting issues for:\")\n",
        "            if problem_drivers: print(f\"     Drivers: {list(problem_drivers)}\")\n",
        "            if problem_products: print(f\"     Products: {list(problem_products)}\")\n",
        "\n",
        "            # Visualize on-time rates again, but highlight problem entities\n",
        "            df_problem_drivers = self.df[self.df['driver_identifier'].isin(list(problem_drivers))]\n",
        "            df_problem_products = self.df[self.df['product'].isin(list(problem_products))]\n",
        "\n",
        "            if not df_problem_drivers.empty:\n",
        "                driver_performance = self.df.groupby('driver_identifier')['on_time'].mean().reset_index()\n",
        "                driver_performance['highlight'] = driver_performance['driver_identifier'].apply(lambda x: 'Problem Driver' if x in problem_drivers else 'Other Drivers')\n",
        "                fig_drivers = px.bar(driver_performance.sort_values(by='on_time', ascending=True),\n",
        "                                     x='driver_identifier', y='on_time',\n",
        "                                     color='highlight', color_discrete_map={'Problem Driver': 'red', 'Other Drivers': 'blue'},\n",
        "                                     title='Driver Performance with Problem Drivers Highlighted',\n",
        "                                     labels={'on_time': 'On-Time Delivery Rate', 'driver_identifier': 'Driver ID'})\n",
        "                fig_drivers.update_layout(xaxis_tickangle=-45)\n",
        "                fig_drivers.show()\n",
        "\n",
        "            if not df_problem_products.empty:\n",
        "                product_performance = self.df.groupby('product')['on_time'].mean().reset_index()\n",
        "                product_performance['highlight'] = product_performance['product'].apply(lambda x: 'Problem Product' if x in problem_products else 'Other Products')\n",
        "                fig_products = px.bar(product_performance.sort_values(by='on_time', ascending=True),\n",
        "                                     x='product', y='on_time',\n",
        "                                     color='highlight', color_discrete_map={'Problem Product': 'red', 'Other Products': 'blue'},\n",
        "                                     title='Product Performance with Problem Products Highlighted',\n",
        "                                     labels={'on_time': 'On-Time Delivery Rate', 'product': 'Product'})\n",
        "                fig_products.update_layout(xaxis_tickangle=-45)\n",
        "                fig_products.show()\n",
        "        else:\n",
        "            print(\"   No specific problem entities to highlight based on current problem analysis.\")\n",
        "\n",
        "    def generate_all_visualizations(self):\n",
        "        \"\"\"Orchestrates the generation and display of all defined plots.\"\"\"\n",
        "        print(\"\\nüöÄ Generating All Visualizations...\")\n",
        "        self._plot_product_quantity_distribution()\n",
        "        self._plot_on_time_rates('origin_warehouse', 'Origin Warehouse')\n",
        "        self._plot_on_time_rates('driver_identifier', 'Driver')\n",
        "        self._plot_problem_hotspots()\n",
        "        print(\"\\n‚úÖ All visualizations generated.\")\n",
        "\n",
        "# Instantiate the VisualizationAgent\n",
        "visualization_agent = VisualizationAgent(preprocessed_df, dataset_context, problem_analysis)\n",
        "\n",
        "# Call the method to generate all visualizations\n",
        "visualization_agent.generate_all_visualizations()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä VisualizationAgent Initialized.\n",
            "\n",
            "üöÄ Generating All Visualizations...\n",
            "Generating Product Quantity Distribution plot...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"8f9e6c64-6d18-407f-a1ab-c8cfb565755d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8f9e6c64-6d18-407f-a1ab-c8cfb565755d\")) {                    Plotly.newPlot(                        \"8f9e6c64-6d18-407f-a1ab-c8cfb565755d\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"Product Quantity=%{x}\\u003cbr\\u003ecount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[59,28,63,47,73,35,5,85,34,88,72,15,68,58,40,71,76,82,58,83,11,34,14,29,31,57,38,48,93,82,46,8,77,70,71,52,52,62,23,75,17,13,13,97,31,15,89,89,52,9,92,49,88,71,95,42,49,67,80,86,50,46,92,78,60,99,98,93,61,21,9,99,45,21,41,38,88,88,41,17,21,75,8,42,90,80,61,87,87,14,55,41,44,70,43,16,25,26,40,42,84,14,25,39,16,95,54,20,7,35],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Product Quantity\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Distribution of Product Quantity\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8f9e6c64-6d18-407f-a1ab-c8cfb565755d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating On-Time Delivery Rates by origin_warehouse plot...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4ba4499d-5014-47f0-9c1f-c6bb007352c2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4ba4499d-5014-47f0-9c1f-c6bb007352c2\")) {                    Plotly.newPlot(                        \"4ba4499d-5014-47f0-9c1f-c6bb007352c2\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Origin Warehouse=%{x}\\u003cbr\\u003eOn-Time Delivery (%)=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"02e5081e-abf3-4e23-a9ef-47b0949f31b9\",\"22b17302-3ea2-4bf4-b5a9-50b4efd0bbc7\",\"2ba7bb1e-3a26-4638-850b-af1689dbf1f8\",\"2ea0f049-e73e-4506-805c-4f12b4a7810b\",\"2581514a-9e27-4d03-9943-478f2e059151\",\"71a28ff5-ef5d-4279-b9b7-8afa7174f579\",\"6820af07-1d42-417d-97a3-176e30884c33\",\"68da2756-09f4-49fa-b3d9-f6b8798da0ac\",\"70aee49a-32ac-4b29-9f05-59de1c043a48\",\"6ae04f04-85df-4b89-b057-5cb0459baab7\",\"4ece87fd-d687-4fa1-a217-7822923a1ade\",\"52075ab4-f664-416f-b583-a366b168bc5e\",\"5edca4f4-c65a-4936-8c63-0694ba8245cb\",\"4684e7d3-c044-4a98-80e9-5f43e9ff23d3\",\"7d12aa83-b1c4-4c6c-a3e6-b7cd6f26498c\",\"742df199-0e51-44d2-93ba-a5e3885d79ed\",\"7103af48-0693-401e-a8a9-7063ff0d0722\",\"d247fff7-d785-4593-aad9-ddb3f6d0a809\",\"d7d52dc6-d13e-4a07-aa6a-934642b20473\",\"d5566b15-b071-4acf-8e8e-c98433083b2d\",\"d45c1fc9-2b06-4a2b-9625-7a8bda5b2be7\",\"d2ee1b75-2218-4753-9487-dcca23d667c6\",\"eb1d7193-5c90-4fcf-b41b-a6a3d5f925df\",\"ff60676a-e25d-4628-84e9-203c98225944\",\"f47e4ef6-e326-433f-884c-a3802095a502\",\"f4372224-759f-43b3-bc83-ca6106bba1af\",\"e92bc306-314e-4853-8728-0107aa27b936\",\"ee0f4141-13ad-4289-a414-f1756a5748ef\",\"e2ee7c06-c34e-42e7-8e35-055953addca3\",\"c2503d64-0050-4d35-822c-6a540272fe1a\",\"b145f396-de9b-42f1-9cc9-f5b52c3a941c\",\"c42e63e2-e613-45df-9321-09c58bcedcf4\",\"b7f83fe7-6ae2-4e75-81e1-8440d26778c2\",\"b7ae2648-bc18-4b26-87cf-8f68b75a7888\",\"a6174fda-397f-4e9a-9fed-75166bca2c16\",\"a3e21e04-b65b-47da-845a-4cdaf559f5f2\",\"a7523398-8ffe-49bb-bec2-ba85709bd3d9\",\"a8420740-3acf-4f09-8564-b2c1182b2299\",\"9a6ee454-f8ca-4a5a-b751-402ce8257b5b\",\"9b878a0f-d9e2-47be-bfd2-2fc381dba95a\",\"8dbbc936-6d70-46c8-8573-c0ca8825329f\",\"8d73d024-6813-44e6-93f2-9ee8cb20718f\",\"8826333b-cf1f-44ee-98db-cc159e5afa5f\",\"84003744-296e-4294-86a3-8a96439d4ee8\",\"80c0f4e4-3001-4983-91a9-44b1742e6f6b\",\"7e79d23c-8877-4200-aeee-35a5934db1df\",\"c47b1521-0d18-410e-908b-780e91a78118\",\"c7f14ee0-3187-4d74-8849-714c10e168ba\",\"c42f0de8-b4f0-4167-abd1-ae79e5e18eea\",\"7cd8ea7c-cea3-46fa-a913-724d59233a43\",\"72ea2fbd-b7e1-4c61-8292-a8924d6ee8f0\",\"7b346de1-fe35-4204-93d2-0cb2600b4213\",\"75891066-59b4-437b-951f-ec553fb26b94\",\"73608d6a-cc61-4043-b95c-175ba478ba7c\",\"48a1b6eb-bac0-4fe8-8dc0-dbf56a49a771\",\"458efede-2515-46b3-87a4-9fd49e2d711b\",\"446119d3-b86d-4648-93e1-7c9104fe6462\",\"49d0edae-9091-41bb-a08d-ab1c66bd08d5\",\"53cfa054-aabc-41b6-a357-af13f2dc5770\",\"68dc7343-9c24-4c9b-bd29-acae05bf399c\",\"6a6d3fce-c5aa-4154-a6a3-b56cb41f709f\",\"6af1f98f-7555-430a-9d7f-d0f70f1b0a54\",\"42929165-b971-4546-b882-8f90e59c3e0f\",\"43e66a58-b6ef-40da-ac74-6516dd0d6830\",\"3f9c4c5a-b644-40ed-8c3e-4776bdcac303\",\"44080c04-3ad0-43bb-a40d-d37e35e032f2\",\"23924ca0-acae-4a6a-aa64-8d47b169f078\",\"2316da83-4a83-497c-bdf4-55ca9e8cadb9\",\"333bc679-0295-49cd-ad2e-cbb89a65bd99\",\"0095d8da-3246-4f33-b76d-990b5ac46dc9\",\"1be3c9a2-37b8-4e24-a360-c4c7cc7ff713\",\"125958b0-d8fd-4dcb-826b-a5f8f39a9956\",\"11382b86-0769-4f6e-afea-c921397a444a\",\"0338aff7-ed92-4128-9957-24e109b1aa5b\",\"39053f90-0c8f-4e5e-b8b2-4b51c82d7014\",\"5ebd7a1c-2d62-406e-b2b4-c250625e6287\",\"6307c024-1705-4508-b05f-ac96dba72fca\",\"6656251a-ca70-401b-9dc1-5b5d892ea630\",\"c3681bad-d1a9-4cc4-8d26-b00e51c0c7ee\",\"c32eed70-314d-4321-b3f3-ea6c673d1cd7\",\"bad3d1a2-b14d-4efa-8f81-5270ee921ae6\",\"bad8935d-55aa-49aa-81d9-d42c3410b108\",\"b23deb12-0dc3-40c3-889c-8e8d3206dc4a\",\"b26d55e2-eb75-4bce-89cf-5cee095eaeea\",\"b65da068-cfdf-4247-84db-91232e610217\",\"b7392abc-151f-43fe-97d8-aee98f46f246\",\"b21bcc31-0451-4f80-963f-3087de248f8a\",\"b19cec0d-357e-4c6b-9257-8be52b1c71b5\",\"af1a5839-9736-4c7d-b48c-8c3c560bbf02\",\"aa0823cf-e8a7-4500-934a-b18b42036d2e\",\"98811bfd-37d0-4895-bbeb-e8235602abbd\",\"89a6aaff-4a87-400f-915f-9e6f2d263a32\",\"a0d5e405-23fc-4c24-8c7f-bd0b40b3b724\",\"d2a2460e-00d1-41f2-84cc-eba01eb88d75\",\"dc883f1f-7e8f-40bf-a4b0-b41328c675b6\",\"dc85afa8-4888-4e1f-8cdd-470879901108\",\"d808ffd0-d467-4bcc-8449-9e748b4a04ce\",\"d7739dfe-596f-4527-a0ae-83a5b2f32b0d\",\"c6d4e9d1-45a7-4948-9ec6-be687c4b8bba\",\"cebefc0d-2fb9-4e91-94fe-2fb720e8f058\",\"d26316d2-19e2-45c0-9ada-b92ba56a4da2\",\"e2055730-995c-4180-894f-116aec59bbab\",\"eea522eb-17c1-42e2-8e33-c33710b7d329\",\"e9bee4c1-c56a-4e8e-8e91-79d8f6a98b5b\",\"f933a1d6-0b38-427a-9fd0-297d8eb94918\",\"f473d983-eb17-4b62-85fd-76b31540e41d\",\"fb374a17-e2b4-4274-b777-a09cccc8dc7b\",\"fc629195-daf2-465a-91bd-330719735bb1\",\"fd861308-291c-435e-a11b-92cbba182fb4\",\"ff32be02-476b-4616-9dbd-78dd6f3d928b\"],\"xaxis\":\"x\",\"y\":[100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Origin Warehouse\"},\"tickangle\":-45},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"On-Time Delivery (%)\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"On-Time Delivery (%)\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Origin Warehouse On-Time Delivery Percentage\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4ba4499d-5014-47f0-9c1f-c6bb007352c2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating On-Time Delivery Rates by driver_identifier plot...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d9b98a06-2af2-4a53-b161-35f0bb0766f8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d9b98a06-2af2-4a53-b161-35f0bb0766f8\")) {                    Plotly.newPlot(                        \"d9b98a06-2af2-4a53-b161-35f0bb0766f8\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Driver Identifier=%{x}\\u003cbr\\u003eOn-Time Delivery (%)=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"005a3b7f-de08-4cbb-bba0-ccf42e85c4d7\",\"10ca12f9-4189-47a2-9686-6f42303282af\",\"16f851ea-8ddd-42b2-92fe-dd9c1ac89e16\",\"293ccaec-6592-4f04-aae5-3e238fe62614\",\"1edc17f1-afbc-44f1-b00b-a23a36440cd1\",\"1c4bf3f8-5445-4dd7-9375-b744e1204e18\",\"28bb92fd-40da-42e5-8189-11f5b6d5cdc5\",\"5f79b402-655f-4d8e-8ff3-5ef05870e0ad\",\"6b27d6a9-1787-478b-a73a-0ded3ca7e026\",\"71a7de14-c546-4099-929c-af307c0d2127\",\"479d4eb2-6d9e-4750-b8f3-b0a6621423f2\",\"50531c03-d16d-4d40-bed3-d379a3f2a3ca\",\"42b9222f-6a4b-4486-b80a-ee7bc18817f5\",\"37ad3c4e-ccde-419b-8dac-acb976d4c280\",\"73994c93-335d-4b64-820b-f539297fbe9c\",\"72ddf0d2-1d52-46fb-a0db-e118386db6d7\",\"76570022-a237-4cad-95fc-7682d2374450\",\"5ab09603-441e-460a-908c-1db3d8641532\",\"c525a67b-72d8-49ba-8c08-3b82635d6f10\",\"ec1fd01b-3869-4ca4-b83e-3ae985bd8db0\",\"ee97c346-fa71-4b90-a2fa-e7c573cca833\",\"d8da0460-cf39-4f38-9fff-6c9b4e344d8a\",\"f7de33d1-bee5-4285-b618-ed65a9a49d3e\",\"fdde8145-7afe-4802-a8df-ebcc090683a6\",\"e3a82c33-1ad1-4bd8-bf70-fa8045e9e7aa\",\"bde715f4-a3b5-497a-bfa0-96a1fac31cef\",\"9d5ee3ca-a053-4ad1-b795-33b9e5d6195e\",\"aa796afa-0ae5-4d17-bdf9-c6391ad880ec\",\"a9784b8d-d222-4cdf-93fb-b3886c8033c5\",\"a50960f2-95fd-4dbc-8ef0-15bf753a20fa\",\"91e1b866-b671-4e0b-aeda-cfc8cab0d86b\",\"92ecc03e-6c20-440c-ac99-c141b17dea8c\",\"99b12083-d781-411d-95e8-9c83dd987099\",\"970801bf-4d8b-445d-8271-447b5e7a5e28\",\"a00bc9eb-ddf2-4ba2-8a9f-c14b15748420\",\"9f563356-28f7-4353-b4b9-8caa64c3fab5\",\"9e292324-8448-4254-8c0a-b0e373aaeaee\",\"8dd4d537-b69a-4d83-91b6-5c20514c74ca\",\"85e1286d-79a7-4308-bbea-2f67a344dad7\",\"8603e148-2d3c-422a-9b36-3a4241bbdce0\",\"892aa333-32c6-41dd-937d-1c2aae5bfdf2\",\"8c9c7567-c853-4769-a8af-244e5373676b\",\"8d322706-d9fc-4ed2-92a3-cec0b186a9c7\",\"80988f09-91a3-4e1b-8e69-13551c53f318\",\"7be9d33a-5c2d-4477-a0d4-ae3a2e0b5fa8\",\"79a8e542-dbc1-4e67-9b4a-98df8cd1b3cc\",\"b55f381d-c18f-4697-8de3-87b2bf6c2144\",\"b85f5ea6-6f33-4746-b933-8a5153a2f142\",\"b06867f3-f39a-4aa7-8e7a-4f774244331b\",\"7321f568-ed5a-4e28-80fb-7f65aebbed0b\",\"776b6a91-ebb0-41b5-a96f-61d2c94720e8\",\"75f6e13d-1f96-474e-84ac-83afbc76b9f6\",\"74575acb-3f78-4b6e-97aa-86823c8daa9b\",\"68f7f701-4d57-4f7f-86fa-dcb679343924\",\"5f814eda-c186-4fc3-ad42-ba5406460198\",\"419b432e-a852-4f94-a881-9c182660e10f\",\"44e05f20-74a0-4ae3-8683-53abac4d66a8\",\"443e39f6-2d29-41e0-b32d-8e7be788dfc6\",\"623d681f-8bc8-4fdf-8045-4f204e8a289a\",\"629b433c-3eef-4338-8714-f899ad79850b\",\"67fc5885-d261-407e-a889-e5bb720d19d5\",\"58beb5d3-98f8-4077-a964-1f04f7cb11e5\",\"2fd9a976-bac5-4803-be43-bf93cc618ad1\",\"31a3a354-b88a-46ef-83bb-4dba05221f8d\",\"3e8e5e4a-8bd6-47a5-8916-7b2f9691b862\",\"31b75be6-53d6-459e-b90f-005dce535a1b\",\"42cf0f3d-6878-4eee-a989-4975261e65dd\",\"1c65b78e-4c67-416e-ad41-70eb4f657710\",\"2587368e-5b2b-48e0-8041-1f757876866b\",\"2aac468c-cd83-4265-bebd-df004337ed90\",\"1eb6eaa0-f3a4-42d0-8e80-505ec78db4c7\",\"1b6f801b-35a7-4d10-a5e3-50ed90c6bd19\",\"150ce0b1-05f8-48ef-bca1-5b516237cfdf\",\"1510dba4-3023-4d43-a92d-edd2092f79df\",\"45c9bd5b-caf6-4ec1-b1eb-09fe615fbdc6\",\"4bd802cd-5b6c-4725-bb63-57b30a411424\",\"5423481d-6fce-40ab-bea9-69640a650b0b\",\"7fe0c723-2b05-48f5-a22c-eb3a60eae33a\",\"a5647bc7-6e55-4163-a97b-5f85f221eb56\",\"95fba593-e6c7-48ff-b1f5-88972f3e2f14\",\"9891cbae-3c02-4f71-97ca-bac2167c2d63\",\"99b624a1-e072-48c6-b427-0b134ca63866\",\"96a9547b-b6bc-4967-9b33-62cbf33e15d0\",\"802722fe-527d-4ddf-91f5-65b2fe1f6dec\",\"8cd769fd-383c-48fd-9553-a53d8215e1b8\",\"8f7daff2-620b-4fc1-9490-a932cebdbd8f\",\"7dbd3510-9735-4bec-8efb-a9ecbc60b27f\",\"81d7a772-5f57-4bc6-9d7f-eadc7ab6b32b\",\"7fe382b8-9316-411f-98e2-c3795e8e32ee\",\"b7aff472-e4a8-4985-b87a-5bc17b8a4984\",\"b7bdc199-8f95-4c3f-a01f-ef484ff10838\",\"af9de98a-175e-438f-9759-c01158885db3\",\"b3aafbd7-9531-4b55-9b04-581821c85e30\",\"abe49e2f-03ce-43f8-b828-79a0905b6327\",\"c7e65ac2-a424-4cdd-abbe-45051dedb3e7\",\"c5302129-b423-4310-a2af-734c04e75955\",\"c51ac282-1e36-4f1f-b624-d1fd3616983c\",\"bfcd12fc-e848-41c6-a891-61cf7469128a\",\"b8d574a5-546c-40c8-8b3b-dad0ff5970b0\",\"ba911cf2-5b62-4b5c-bca7-ab69c3a06c6b\",\"d34249fa-8f2a-4052-83ba-7118122d5e34\",\"cc022d6c-8172-432b-86fc-7669ccc01030\",\"d95cf3ae-f5e5-487b-947c-f628d0a44d30\",\"d7432792-20ad-4a7f-a395-81f04fee89fe\",\"d6e311be-6352-4a0d-8221-576d527953d4\",\"d684dfe0-666f-4e9d-9c2e-8d74f68fe684\",\"cebc86e8-c327-46f7-96b3-35684d169455\",\"eb398b83-36f2-4d40-91fd-26e086c5c9cf\",\"f521809c-bc9b-4117-b58b-d8252ffbf717\",\"f45a9c70-7c12-47a5-bf01-5aea93120d69\"],\"xaxis\":\"x\",\"y\":[100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,100.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Driver Identifier\"},\"tickangle\":-45},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"On-Time Delivery (%)\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"On-Time Delivery (%)\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Driver On-Time Delivery Percentage\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d9b98a06-2af2-4a53-b161-35f0bb0766f8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Problem Hotspots visualization...\n",
            "   No specific problem entities to highlight based on current problem analysis.\n",
            "\n",
            "‚úÖ All visualizations generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c91bd60f",
        "outputId": "dfc3883f-cf7b-4e9f-c24f-07b79ce33c38"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict\n",
        "\n",
        "class SessionService(ABC):\n",
        "    \"\"\"Abstract base class defining the interface for session management.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_session(self, session_id: str) -> Dict:\n",
        "        \"\"\"Retrieves session data for a given session ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def update_session(self, session_id: str, data: Dict) -> None:\n",
        "        \"\"\"Updates session data for a given session ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def delete_session(self, session_id: str) -> None:\n",
        "        \"\"\"Deletes a session for a given session ID.\"\"\"\n",
        "        pass\n",
        "\n",
        "print(\"Abstract base class 'SessionService' defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract base class 'SessionService' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cfbb7c5",
        "outputId": "f1fd8f3b-be38-4ba8-c2ec-31461359ec82"
      },
      "source": [
        "class InMemorySessionService(SessionService):\n",
        "    \"\"\"Concrete implementation of SessionService using an in-memory dictionary.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sessions = {}\n",
        "        print(\"Initialized InMemorySessionService with an empty session store.\")\n",
        "\n",
        "    def get_session(self, session_id: str) -> Dict:\n",
        "        \"\"\"Retrieves session data for a given session ID.\"\"\"\n",
        "        return self.sessions.get(session_id, {})\n",
        "\n",
        "    def update_session(self, session_id: str, data: Dict) -> None:\n",
        "        \"\"\"Updates session data for a given session ID.\"\"\"\n",
        "        self.sessions[session_id] = data\n",
        "        print(f\"Session '{session_id}' updated.\")\n",
        "\n",
        "    def delete_session(self, session_id: str) -> None:\n",
        "        \"\"\"Deletes a session for a given session ID.\"\"\"\n",
        "        if session_id in self.sessions:\n",
        "            del self.sessions[session_id]\n",
        "            print(f\"Session '{session_id}' deleted.\")\n",
        "        else:\n",
        "            print(f\"Session '{session_id}' not found for deletion.\")\n",
        "\n",
        "print(\"'InMemorySessionService' implemented.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'InMemorySessionService' implemented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdfe0094",
        "outputId": "4ce37c2c-7ed7-419e-c05e-6d245aec295d"
      },
      "source": [
        "class DatasetIntelligenceAgent:\n",
        "    \"\"\"Understands what kind of data we're analyzing\"\"\"\n",
        "\n",
        "    def __init__(self, session_service: SessionService, session_id: str):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.analysis = None\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "\n",
        "        # Attempt to retrieve previous analysis from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'dataset_analysis' in session_data:\n",
        "            self.analysis = session_data['dataset_analysis']\n",
        "            print(f\"Retrieved dataset analysis from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous dataset analysis found in session {self.session_id}.\")\n",
        "\n",
        "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Deep analysis of dataset structure and business context\"\"\"\n",
        "\n",
        "        if self.analysis is not None: # Use cached analysis if available\n",
        "            print(\"Using cached dataset analysis.\")\n",
        "            return self.analysis\n",
        "\n",
        "        # Prepare dataset summary\n",
        "        summary = {\n",
        "            \"columns\": list(df.columns),\n",
        "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
        "            \"row_count\": len(df),\n",
        "            \"sample_data\": df.head(5).to_dict('records'),\n",
        "            \"null_counts\": df.isnull().sum().to_dict(),\n",
        "            \"unique_counts\": {col: df[col].nunique() for col in df.columns}\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a business dataset. Determine what domain this belongs to and how to analyze it.\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- Columns: {summary['columns']}\n",
        "- Data types: {summary['dtypes']}\n",
        "- Row count: {summary['row_count']}\n",
        "- Unique value counts: {summary['unique_counts']}\n",
        "- Sample rows: {json.dumps(summary['sample_data'][:3], indent=2, default=str)}\n",
        "\n",
        "TASK: Analyze this dataset and provide a structured understanding.\n",
        "\n",
        "Return ONLY a JSON object with this structure:\n",
        "{{\n",
        "  \"domain\": \"string (e.g., 'supply_chain', 'sales', 'inventory', 'logistics', 'retail')\",\n",
        "  \"dataset_type\": \"string (e.g., 'shipment_tracking', 'order_fulfillment', 'warehouse_inventory')\",\n",
        "  \"business_context\": \"string (brief description of what this data represents)\",\n",
        "  \"key_entities\": [\"list\", \"of\", \"main\", \"entities\"],\n",
        "  \"problem_indicators\": {{\n",
        "    \"column_name\": \"what problem it indicates\"\n",
        "  }},\n",
        "  \"success_metrics\": {{\n",
        "    \"column_name\": \"what success it measures\"\n",
        "  }},\n",
        "  \"potential_issues\": [\"list\", \"of\", \"issues\", \"to\", \"monitor\"],\n",
        "  \"autonomous_actions\": [\"list\", \"of\", \"actions\", \"an\", \"agent\", \"could\", \"take\"],\n",
        "  \"analysis_strategy\": \"how to approach analyzing this data\"\n",
        "}}\n",
        "\n",
        "Be specific based on actual column names and data patterns.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        self.analysis = clean_json_response(response_text)\n",
        "\n",
        "        # Store the analysis in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['dataset_analysis'] = self.analysis\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored dataset analysis in session {self.session_id}.\")\n",
        "\n",
        "        print(\"\\nü§ñ AI Dataset Analysis:\")\n",
        "        print(json.dumps(self.analysis, indent=2))\n",
        "\n",
        "        return self.analysis\n",
        "\n",
        "# Re-instantiate and run dataset analysis with session service\n",
        "intelligence_agent = DatasetIntelligenceAgent(session_service, session_id)\n",
        "dataset_context = intelligence_agent.analyze_dataset(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved dataset analysis from session analysis_session_20251129_151936.\n",
            "Using cached dataset analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "219f85f5",
        "outputId": "d9be9cb7-5fe5-488c-cb91-ca0cdd033a48"
      },
      "source": [
        "class ProblemDetectionAgent:\n",
        "    \"\"\"Scans data for issues requiring intervention\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict, session_service: SessionService, session_id: str):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "        self.problems_found = []\n",
        "        self.problem_analysis_report = None\n",
        "\n",
        "        # Attempt to retrieve previous problem analysis from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'problem_analysis_report' in session_data:\n",
        "            self.problem_analysis_report = session_data['problem_analysis_report']\n",
        "            self.problems_found = self.problem_analysis_report.get('problems', [])\n",
        "            print(f\"Retrieved problem analysis from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous problem analysis found in session {self.session_id}.\")\n",
        "\n",
        "    def scan_dataset(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Analyze entire dataset for patterns and problems\"\"\"\n",
        "\n",
        "        if self.problem_analysis_report is not None: # Use cached analysis if available\n",
        "            print(\"Using cached problem analysis.\")\n",
        "            return self.problem_analysis_report\n",
        "\n",
        "        # Create aggregated view for pattern detection\n",
        "        analysis_data = {\n",
        "            \"total_rows\": len(df),\n",
        "            \"problem_indicators\": self.context.get(\"problem_indicators\", {}),\n",
        "            \"column_stats\": {}\n",
        "        }\n",
        "\n",
        "        # Calculate relevant statistics\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"mean\": float(df[col].mean()),\n",
        "                    \"std\": float(df[col].std()),\n",
        "                    \"min\": float(df[col].min()),\n",
        "                    \"max\": float(df[col].max())\n",
        "                }\n",
        "            elif df[col].dtype == 'bool':\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"true_count\": int(df[col].sum()),\n",
        "                    \"false_count\": int((~df[col]).sum()),\n",
        "                    \"true_percentage\": float(df[col].mean() * 100)\n",
        "                }\n",
        "            else:\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"unique_count\": int(df[col].nunique()),\n",
        "                    \"top_values\": df[col].value_counts().head(5).to_dict()\n",
        "                }\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a {self.context['dataset_type']} dataset to find problems.\n",
        "\n",
        "DATASET CONTEXT:\n",
        "{json.dumps(self.context, indent=2)}\n",
        "\n",
        "STATISTICAL ANALYSIS:\n",
        "{json.dumps(analysis_data, indent=2, default=str)}\n",
        "\n",
        "SAMPLE RECORDS:\n",
        "{json.dumps(df.head(10).to_dict('records'), indent=2, default=str)}\n",
        "\n",
        "TASK: Identify specific, actionable problems that require autonomous intervention.\n",
        "\n",
        "Return ONLY a JSON object:\n",
        "{{\n",
        "  \"problems\": [\n",
        "    {{\n",
        "      \"problem_id\": \"PROB-001\",\n",
        "      \"severity\": \"CRITICAL|HIGH|MEDIUM|LOW\",\n",
        "      \"category\": \"category name\",\n",
        "      \"description\": \"specific problem description\",\n",
        "      \"affected_records\": \"how many/which records\",\n",
        "      \"business_impact\": \"what's the business impact\",\n",
        "      \"requires_action\": true/false,\n",
        "      \"suggested_actions\": [\"action1\", \"action2\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"summary\": {{\n",
        "    \"total_problems\": 0,\n",
        "    \"critical_count\": 0,\n",
        "    \"high_count\": 0,\n",
        "    \"requires_immediate_action\": true/false\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Focus on REAL problems in the data, not hypothetical ones.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        result = clean_json_response(response_text)\n",
        "\n",
        "        self.problems_found = result.get(\"problems\", [])\n",
        "        self.problem_analysis_report = result\n",
        "\n",
        "        # Store the problem analysis in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['problem_analysis_report'] = self.problem_analysis_report\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored problem analysis in session {self.session_id}.\")\n",
        "\n",
        "        print(f\"\\n‚ö†Ô∏è  PROBLEM SCAN COMPLETE\")\n",
        "        print(f\"   Total Problems Found: {result['summary']['total_problems']}\")\n",
        "        print(f\"   Critical: {result['summary']['critical_count']}\")\n",
        "        print(f\"   High: {result['summary']['high_count']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# Run problem detection with session service\n",
        "problem_detector = ProblemDetectionAgent(dataset_context, session_service, session_id)\n",
        "problem_analysis = problem_detector.scan_dataset(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous problem analysis found in session analysis_session_20251129_151936.\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored problem analysis in session analysis_session_20251129_151936.\n",
            "\n",
            "‚ö†Ô∏è  PROBLEM SCAN COMPLETE\n",
            "   Total Problems Found: 3\n",
            "   Critical: 0\n",
            "   High: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74f0c9bd"
      },
      "source": [
        "import time\n",
        "from datetime import datetime, timezone\n",
        "import json\n",
        "import requests # Import the requests library to catch its exceptions\n",
        "\n",
        "def utc_now():\n",
        "    \"\"\"Get current UTC timestamp\"\"\"\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def safe_api_call(model, prompt, max_retries=5): # Increased max_retries for more robustness\n",
        "    \"\"\"Make API call with retry logic, including connection errors\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except (requests.exceptions.ConnectionError, requests.exceptions.RequestException, Exception) as e: # Catch ConnectionError and general RequestException\n",
        "            if \"429\" in str(e) or \"quota\" in str(e).lower() or \"Connection aborted\" in str(e):\n",
        "                wait_time = (attempt + 1) * 5 # Increased wait time for connection issues\n",
        "                print(f\"‚è≥ API call failed (Attempt {attempt + 1}/{max_retries}). Retrying after {wait_time}s due to: {e}\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"‚ùå Unhandled API error: {e}\")\n",
        "                raise e\n",
        "    raise Exception(f\"Max retries ({max_retries}) exceeded for API call.\")\n",
        "\n",
        "def clean_json_response(text):\n",
        "    \"\"\"Extract and clean JSON from AI response\"\"\"\n",
        "    text = text.strip()\n",
        "    # Remove markdown code blocks\n",
        "    text = text.replace('```json', '').replace('```', '')\n",
        "    # Find JSON object boundaries\n",
        "    start = text.find('{')\n",
        "    end = text.rfind('}') + 1\n",
        "    if start != -1 and end > start:\n",
        "        text = text[start:end]\n",
        "    return json.loads(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3cbfe94e",
        "outputId": "846acdd8-892e-4cd2-9c61-ac9a4c93b77f"
      },
      "source": [
        "class ExecutionAgent:\n",
        "    \"\"\"Simulates execution of autonomous plans\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict, session_service: SessionService, session_id: str):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "        self.execution_logs = []\n",
        "\n",
        "        # Attempt to retrieve previous execution results from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'execution_results' in session_data:\n",
        "            self.execution_logs = session_data['execution_results']\n",
        "            print(f\"Retrieved {len(self.execution_logs)} execution logs from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous execution logs found in session {self.session_id}.\")\n",
        "\n",
        "    def execute_plan(self, plan: Dict) -> Dict:\n",
        "        \"\"\"Execute each step of the plan\"\"\"\n",
        "\n",
        "        print(f\"\\n‚Ñõ EXECUTING: {plan['plan_id']}\")\n",
        "        print(f\"   Objective: {plan['objective']}\")\n",
        "\n",
        "        execution_log = {\n",
        "            \"plan_id\": plan['plan_id'],\n",
        "            \"started_at\": utc_now(),\n",
        "            \"steps_executed\": [],\n",
        "            \"status\": \"IN_PROGRESS\"\n",
        "        }\n",
        "\n",
        "        for i, step in enumerate(plan['execution_steps']):\n",
        "            step_number = step.get('step_number', i + 1) # Use existing step_number or assign based on index\n",
        "            print(f\"\\n   Step {step_number}: {step['description']}\")\n",
        "\n",
        "            # Simulate execution with AI\n",
        "            prompt = f\"\"\"Simulate executing this autonomous action:\\n\\nSTEP: {json.dumps(step, indent=2)}\\n\\nCONTEXT: {self.context['dataset_type']} system\\n\\nSimulate realistic execution and return result in JSON:\\n{{\\n  \"step_number\": {step_number},\\n  \"status\": \"SUCCESS|FAILED|PARTIAL\",\\n  \"action_taken\": \"specific action performed\",\\n  \"result_data\": {{}},\\n  \"metrics\": {{}},\\n  \"notes\": \"important details\",\\n  \"timestamp\": \"{utc_now()}\"\\n}}\\n\\nBe realistic about what would actually happen.\"\"\"\n",
        "\n",
        "            response_text = safe_api_call(self.model, prompt)\n",
        "            result = clean_json_response(response_text)\n",
        "\n",
        "            execution_log[\"steps_executed\"].append(result)\n",
        "\n",
        "            status_icon = \"‚úì\" if result[\"status\"] == \"SUCCESS\" else \"‚úó\"\n",
        "            print(f\"      {status_icon} {result['status']}: {result['action_taken']}\")\n",
        "\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "        execution_log[\"completed_at\"] = utc_now()\n",
        "        execution_log[\"status\"] = \"COMPLETED\" if all(\n",
        "            s[\"status\"] == \"SUCCESS\" for s in execution_log[\"steps_executed\"]\n",
        "        ) else \"PARTIAL\"\n",
        "\n",
        "        self.execution_logs.append(execution_log)\n",
        "\n",
        "        # Store the updated execution logs list in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['execution_results'] = self.execution_logs\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored updated execution logs in session {self.session_id}.\")\n",
        "\n",
        "        print(f\"\\n   ‚úÖ Plan {execution_log['status']}\")\n",
        "\n",
        "        return execution_log\n",
        "\n",
        "# Execute all plans with session service\n",
        "executor_agent = ExecutionAgent(dataset_context, session_service, session_id)\n",
        "execution_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ AUTONOMOUS EXECUTION STARTED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for plan in planner_agent.plans: # Use plans from planner_agent which are already in session\n",
        "    result = executor_agent.execute_plan(plan)\n",
        "    execution_results.append(result)\n",
        "    time.sleep(2)  # Rate limiting between plans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous execution logs found in session analysis_session_20251129_151936.\n",
            "\n",
            "============================================================\n",
            "üöÄ AUTONOMOUS EXECUTION STARTED\n",
            "============================================================\n",
            "\n",
            "‚Ñõ EXECUTING: PLAN-PROB-001\n",
            "   Objective: To significantly improve the on-time delivery rate by identifying and addressing the root causes of delivery delays.\n",
            "\n",
            "   Step 1: Analyze the on-time delivery percentage by origin_warehouse to identify warehouses with a higher incidence of delays.\n",
            "      ‚úì SUCCESS: Executed SQL query on the 'deliveries' table.\n",
            "\n",
            "   Step 2: Analyze the on-time delivery percentage by destination_store to identify stores that are frequently experiencing late deliveries.\n",
            "      ‚úì SUCCESS: Executed SQL query against the shipment_delivery_performance database.\n",
            "\n",
            "   Step 3: Analyze the on-time delivery percentage by product to identify products that are more prone to delays.\n",
            "      ‚úì SUCCESS: Executed SQL query against the 'deliveries' table in the shipment_delivery_performance system's SQL Database.\n",
            "\n",
            "   Step 4: Analyze the on-time delivery percentage by driver_identifier to identify drivers with a consistently high incidence of late deliveries.\n",
            "      ‚úì SUCCESS: Executed SQL query to calculate on-time delivery percentage per driver.\n",
            "\n",
            "   Step 5: Flag drivers with a below-average on-time delivery rate for performance review.\n",
            "      ‚úì SUCCESS: Called the `driver_performance_api.flag_for_review` endpoint.\n",
            "\n",
            "   Step 6: Investigate root causes for delays to specific destination stores identified as problematic in step 2.\n",
            "      ‚úì SUCCESS: Called delivery_investigation_api.investigate_destination_store_delays with target_stores=['Store_A', 'Store_B', 'Store_C', 'Store_D', 'Store_E'].\n",
            "\n",
            "   Step 7: Identify products with a high incidence of 'not on_time' deliveries, as identified in step 3.\n",
            "      ‚úì SUCCESS: Called product_performance_api.identify_problematic_products with product_list=['SKU123', 'SKU456', 'SKU789', 'SKU012', 'SKU345']\n",
            "\n",
            "   Step 8: Alert management to significant deviations in 'on_time' delivery rates and the findings from the analysis.\n",
            "      ‚úì SUCCESS: email_notification_service.send_email(recipient='management@logistics.com', subject='Urgent: Significant Delivery Delay Issues Identified (PROB-001)', body='Analysis of delivery data reveals a critical on-time delivery rate of 44.55%. Key contributing factors have been identified by origin warehouse, destination store, product, and driver. Please review attached summary report for details and recommended actions.')\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated execution logs in session analysis_session_20251129_151936.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n",
            "\n",
            "‚Ñõ EXECUTING: PLAN-PROB-002\n",
            "   Objective: To determine if 'capes' have a recurring late delivery issue and to identify other high-frequency products with similar delivery performance problems.\n",
            "\n",
            "   Step 1: Calculate the on-time delivery rate specifically for the product 'capes'. This involves querying delivery records for 'capes' and counting instances of 'on_time' vs. 'not on_time' deliveries.\n",
            "      ‚úì SUCCESS: Executed SQL query to calculate on-time delivery rate for 'capes'.\n",
            "\n",
            "   Step 2: Identify other high-frequency products. This involves grouping deliveries by product and counting the occurrences of each product. High-frequency products are defined as those with more than a predefined threshold (e.g., 5 deliveries).\n",
            "      ‚úì SUCCESS: Executed SQL query to identify high-frequency products.\n",
            "\n",
            "   Step 3: For each of the high-frequency products identified in Step 2, calculate their on-time delivery rate. This will involve a similar query as in Step 1, but iterated for each product.\n",
            "      ‚úì SUCCESS: Executed SQL queries against the 'deliveries' table for each high-frequency product identified in Step 2.\n",
            "\n",
            "   Step 4: Alert management to deviations in 'on_time' delivery rates for 'capes' and any other high-frequency products identified with a high rate of late deliveries (e.g., on-time rate below a certain threshold, like 80%).\n",
            "      ‚úì SUCCESS: Sent an email notification to 'management@example.com' regarding delivery performance for 'capes' and other high-frequency products.\n",
            "\n",
            "   Step 5: Identify products with a high incidence of 'not on_time' deliveries. This action is partially covered by Step 3, but this step can be used to formally trigger a more in-depth analysis of any product meeting a predefined threshold for late deliveries.\n",
            "      ‚úì SUCCESS: Called Delivery Performance Analysis API to identify products with a high incidence of 'not on_time' deliveries.\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated execution logs in session analysis_session_20251129_151936.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n",
            "\n",
            "‚Ñõ EXECUTING: PLAN-PROB-003\n",
            "   Objective: To understand and mitigate the high variance in 'product_quantity' shipped to identify the root causes and minimize inventory discrepancies at destination stores.\n",
            "\n",
            "   Step 1: Retrieve historical order data to identify expected product quantities for each shipment.\n",
            "      ‚úì SUCCESS: Executed SQL query against the historical_orders table in the shipment_delivery_performance database.\n",
            "\n",
            "   Step 2: Join the retrieved expected quantities with the shipment performance data to calculate the variance for each product in each shipment.\n",
            "      ‚úì SUCCESS: Executed SQL query to join shipment delivery performance data with historical orders to calculate quantity variance.\n",
            "\n",
            "   Step 3: Analyze the quantity variance by 'product_id' and 'origin_warehouse' to identify specific products or warehouses with disproportionately high variances.\n",
            "      ‚úì SUCCESS: Executed SQL query to analyze quantity variance by product_id and origin_warehouse.\n",
            "\n",
            "   Step 4: Correlate quantity discrepancies with 'on_time' delivery status to understand if delays are contributing to fulfillment errors.\n",
            "      ‚úì SUCCESS: Executed SQL query to correlate delivery timeliness with quantity variance.\n",
            "\n",
            "   Step 5: Generate a summary report of findings from the analysis, highlighting products, origin warehouses, and any correlation with delivery timeliness. This report will be sent to the logistics management team.\n",
            "      ‚úì SUCCESS: EmailNotificationService.send_email\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated execution logs in session analysis_session_20251129_151936.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7624edfb",
        "outputId": "de5c02a8-5ae7-45f8-b6fd-360e3acd43ab"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "class MemoryService(ABC):\n",
        "    \"\"\"Abstract base class defining the interface for a long-term memory system.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def store_information(self, key: str, value: Any, metadata: Dict = None) -> None:\n",
        "        \"\"\"Stores information with a given key, value, and optional metadata.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def retrieve_information(self, key: str) -> Any:\n",
        "        \"\"\"Retrieves information associated with a given key.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def query_information(self, query: str, top_k: int = 1) -> List[Any]:\n",
        "        \"\"\"Queries the memory system based on a natural language query, returning top_k results.\"\"\"\n",
        "        pass\n",
        "\n",
        "print(\"Abstract base class 'MemoryService' defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract base class 'MemoryService' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dff1f84b"
      },
      "source": [
        "## Implement MemoryBank\n",
        "\n",
        "### Subtask:\n",
        "Develop a concrete implementation of the long-term memory interface (MemoryService) using an in-memory dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acd8a65",
        "outputId": "13cdb275-22ff-41f4-dc9d-21a1af31557f"
      },
      "source": [
        "class MemoryBank(MemoryService):\n",
        "    \"\"\"Concrete implementation of MemoryService using an in-memory dictionary.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.memory_store = {}\n",
        "        print(\"Initialized MemoryBank with an empty memory store.\")\n",
        "\n",
        "    def store_information(self, key: str, value: Any, metadata: Dict = None) -> None:\n",
        "        \"\"\"Stores information with a given key, value, and optional metadata.\"\"\"\n",
        "        self.memory_store[key] = {\"value\": value, \"metadata\": metadata if metadata is not None else {}}\n",
        "        print(f\"Information stored for key: '{key}'.\")\n",
        "\n",
        "    def retrieve_information(self, key: str) -> Any:\n",
        "        \"\"\"Retrieves information associated with a given key.\"\"\"\n",
        "        entry = self.memory_store.get(key)\n",
        "        return entry[\"value\"] if entry else None\n",
        "\n",
        "    def query_information(self, query: str, top_k: int = 1) -> List[Any]:\n",
        "        \"\"\"Queries the memory system based on a natural language query.\n",
        "        For this in-memory dictionary, it performs an exact key match.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        if query in self.memory_store:\n",
        "            results.append(self.memory_store[query][\"value\"])\n",
        "        # For simplicity, if top_k > 1 and no exact match, return first top_k items or fewer\n",
        "        elif not results:\n",
        "            all_values = [entry[\"value\"] for entry in self.memory_store.values()]\n",
        "            results.extend(all_values[:top_k])\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"'MemoryBank' implemented.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'MemoryBank' implemented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e852d27"
      },
      "source": [
        "## Integrate Long-Term Memory\n",
        "\n",
        "### Subtask:\n",
        "Integrate MemoryBank into DatasetIntelligenceAgent to store dataset analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "857d785e",
        "outputId": "cbf4bc45-d577-4fbe-e28a-e2e40456a824"
      },
      "source": [
        "memory_service = MemoryBank()\n",
        "print(\"Initialized MemoryBank instance.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized MemoryBank with an empty memory store.\n",
            "Initialized MemoryBank instance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "f055af30",
        "outputId": "8c875327-7b32-43b2-f2ea-41e478cd01f6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Clear previous analysis from the session to force re-generation\n",
        "session_data = session_service.get_session(session_id)\n",
        "if 'dataset_analysis' in session_data:\n",
        "    del session_data['dataset_analysis']\n",
        "    session_service.update_session(session_id, session_data)\n",
        "    print(f\"Cleared 'dataset_analysis' from session {session_id} to force re-analysis.\")\n",
        "\n",
        "class DatasetIntelligenceAgent:\n",
        "    \"\"\"Understands what kind of data we're analyzing\"\"\"\n",
        "\n",
        "    def __init__(self, session_service: SessionService, session_id: str, memory_service: MemoryService):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.analysis = None\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "        self.memory_service = memory_service # Store memory service\n",
        "\n",
        "        # Attempt to retrieve previous analysis from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'dataset_analysis' in session_data:\n",
        "            self.analysis = session_data['dataset_analysis']\n",
        "            print(f\"Retrieved dataset analysis from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous dataset analysis found in session {self.session_id}.\")\n",
        "\n",
        "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Deep analysis of dataset structure and business context\"\"\"\n",
        "\n",
        "        if self.analysis is not None: # Use cached analysis if available\n",
        "            print(\"Using cached dataset analysis.\")\n",
        "            return self.analysis\n",
        "\n",
        "        # Prepare dataset summary\n",
        "        summary = {\n",
        "            \"columns\": list(df.columns),\n",
        "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
        "            \"row_count\": len(df),\n",
        "            \"sample_data\": df.head(5).to_dict('records'),\n",
        "            \"null_counts\": df.isnull().sum().to_dict(),\n",
        "            \"unique_counts\": {col: df[col].nunique() for col in df.columns}\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a business dataset. Determine what domain this belongs to and how to analyze it.\n",
        "\n",
        "DATASET INFORMATION:\n",
        "- Columns: {summary['columns']}\n",
        "- Data types: {summary['dtypes']}\n",
        "- Row count: {summary['row_count']}\n",
        "- Unique value counts: {summary['unique_counts']}\n",
        "- Sample rows: {json.dumps(summary['sample_data'][:3], indent=2, default=str)}\n",
        "\n",
        "TASK: Analyze this dataset and provide a structured understanding.\n",
        "\n",
        "Return ONLY a JSON object with this structure:\n",
        "{{\n",
        "  \"domain\": \"string (e.g., 'supply_chain', 'sales', 'inventory', 'logistics', 'retail')\",\n",
        "  \"dataset_type\": \"string (e.g., 'shipment_tracking', 'order_fulfillment', 'warehouse_inventory')\",\n",
        "  \"business_context\": \"string (brief description of what this data represents)\",\n",
        "  \"key_entities\": [\"list\", \"of\", \"main\", \"entities\"],\n",
        "  \"problem_indicators\": {{\n",
        "    \"column_name\": \"what problem it indicates\"\n",
        "  }},\n",
        "  \"success_metrics\": {{\n",
        "    \"column_name\": \"what success it measures\"\n",
        "  }},\n",
        "  \"potential_issues\": [\"list\", \"of\", \"issues\", \"to\", \"monitor\"],\n",
        "  \"autonomous_actions\": [\"list\", \"of\", \"actions\", \"an\", \"agent\", \"could\", \"take\"],\n",
        "  \"analysis_strategy\": \"how to approach analyzing this data\"\n",
        "}}\n",
        "\n",
        "Be specific based on actual column names and data patterns.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        self.analysis = clean_json_response(response_text)\n",
        "\n",
        "        # Store the analysis in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['dataset_analysis'] = self.analysis\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored dataset analysis in session {self.session_id}.\")\n",
        "\n",
        "        # Store the analysis in long-term memory\n",
        "        memory_key = f\"dataset_analysis_{self.session_id}\"\n",
        "        self.memory_service.store_information(key=memory_key, value=self.analysis, metadata={'session_id': self.session_id, 'timestamp': utc_now()})\n",
        "        print(f\"Stored dataset analysis in MemoryBank with key: {memory_key}.\")\n",
        "\n",
        "        print(\"\\nü§ñ AI Dataset Analysis:\")\n",
        "        print(json.dumps(self.analysis, indent=2))\n",
        "\n",
        "        return self.analysis\n",
        "\n",
        "# Re-loading df for this execution context to resolve NameError\n",
        "# This assumes 'shipping_data_0.csv' is the correct file name and is available in the environment.\n",
        "csv_name = 'shipping_data_0.csv' # Explicitly set based on previous output\n",
        "df = pd.read_csv(csv_name)\n",
        "\n",
        "# Re-instantiate and run dataset analysis with session service and memory service\n",
        "intelligence_agent = DatasetIntelligenceAgent(session_service, session_id, memory_service)\n",
        "dataset_context = intelligence_agent.analyze_dataset(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Cleared 'dataset_analysis' from session analysis_session_20251129_151936 to force re-analysis.\n",
            "No previous dataset analysis found in session analysis_session_20251129_151936.\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored dataset analysis in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'dataset_analysis_analysis_session_20251129_151936'.\n",
            "Stored dataset analysis in MemoryBank with key: dataset_analysis_analysis_session_20251129_151936.\n",
            "\n",
            "ü§ñ AI Dataset Analysis:\n",
            "{\n",
            "  \"domain\": \"logistics\",\n",
            "  \"dataset_type\": \"shipment_delivery_performance\",\n",
            "  \"business_context\": \"This dataset appears to track product shipments from origin warehouses to destination stores, focusing on delivery timeliness, product quantities, and the drivers involved. It is likely used to monitor and improve the efficiency and reliability of the transportation and delivery process.\",\n",
            "  \"key_entities\": [\n",
            "    \"warehouse\",\n",
            "    \"store\",\n",
            "    \"product\",\n",
            "    \"driver\",\n",
            "    \"shipment\"\n",
            "  ],\n",
            "  \"problem_indicators\": {\n",
            "    \"on_time\": \"Indicates delayed or missed deliveries.\",\n",
            "    \"product_quantity\": \"Can indicate stock discrepancies or issues with order fulfillment at either the warehouse or store end.\",\n",
            "    \"origin_warehouse\": \"Unusual patterns of shipments from specific warehouses could indicate capacity issues or operational inefficiencies.\",\n",
            "    \"destination_store\": \"Unusual patterns of shipments to specific stores could indicate demand issues or inventory management problems at the store level.\"\n",
            "  },\n",
            "  \"success_metrics\": {\n",
            "    \"on_time\": \"Measures the percentage of deliveries that arrive on schedule. Higher is better.\",\n",
            "    \"product_quantity\": \"While not a direct success metric, consistent and accurate product quantities are crucial for successful fulfillment. Deviations could indicate problems.\"\n",
            "  },\n",
            "  \"potential_issues\": [\n",
            "    \"High rates of late deliveries.\",\n",
            "    \"Significant discrepancies in product quantities shipped vs. received.\",\n",
            "    \"Bottlenecks or inefficiencies at specific origin warehouses.\",\n",
            "    \"Repeated delivery failures to certain destination stores.\",\n",
            "    \"Driver performance variations impacting delivery times.\",\n",
            "    \"Product-specific delivery challenges.\"\n",
            "  ],\n",
            "  \"autonomous_actions\": [\n",
            "    \"Flag shipments that are trending towards being late and notify relevant stakeholders.\",\n",
            "    \"Identify drivers with consistently poor on-time performance and flag for review or retraining.\",\n",
            "    \"Analyze product delivery patterns to identify high-risk products or routes.\",\n",
            "    \"Alert when product quantities significantly deviate from expected values for a given shipment.\",\n",
            "    \"Recommend re-routing or alternative delivery methods for delayed shipments.\",\n",
            "    \"Identify warehouses or stores with a high incidence of delivery issues for further investigation.\"\n",
            "  ],\n",
            "  \"analysis_strategy\": \"The analysis should focus on understanding delivery performance and identifying areas for improvement. Key steps include:\\n1. **Descriptive Statistics:** Calculate the overall on-time delivery rate. Analyze the distribution of product quantities.\\n2. **Performance by Category:** Analyze on-time delivery rates broken down by origin_warehouse, destination_store, product, and driver_identifier to identify specific areas of concern.\\n3. **Correlation Analysis:** Investigate if there are correlations between product quantity and on-time delivery. (e.g., are larger quantities more prone to delays?)\\n4. **Outlier Detection:** Identify shipments with exceptionally high or low product quantities, or drivers with extreme delivery performance.\\n5. **Trend Analysis (if temporal data were available):** If timestamps were present, analyze trends in delivery performance over time.\\n6. **Root Cause Analysis:** For identified issues (e.g., late deliveries to a specific store), investigate further by looking at related shipments and drivers.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d509f3ba"
      },
      "source": [
        "## Integrate Long-Term Memory into ProblemDetectionAgent\n",
        "\n",
        "### Subtask:\n",
        "Integrate MemoryBank into ProblemDetectionAgent to store detected problems and analysis reports.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03a6f6a1",
        "outputId": "be143cd8-5cc1-4b89-b191-be75da0eaa8d"
      },
      "source": [
        "class ProblemDetectionAgent:\n",
        "    \"\"\"Scans data for issues requiring intervention\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict, session_service: SessionService, session_id: str, memory_service: MemoryService):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "        self.memory_service = memory_service # Store memory service\n",
        "        self.problems_found = []\n",
        "        self.problem_analysis_report = None\n",
        "\n",
        "        # Attempt to retrieve previous problem analysis from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'problem_analysis_report' in session_data:\n",
        "            self.problem_analysis_report = session_data['problem_analysis_report']\n",
        "            self.problems_found = self.problem_analysis_report.get('problems', [])\n",
        "            print(f\"Retrieved problem analysis from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous problem analysis found in session {self.session_id}.\")\n",
        "\n",
        "    def scan_dataset(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Analyze entire dataset for patterns and problems\"\"\"\n",
        "\n",
        "        if self.problem_analysis_report is not None: # Use cached analysis if available\n",
        "            print(\"Using cached problem analysis.\")\n",
        "            return self.problem_analysis_report\n",
        "\n",
        "        # Create aggregated view for pattern detection\n",
        "        analysis_data = {\n",
        "            \"total_rows\": len(df),\n",
        "            \"problem_indicators\": self.context.get(\"problem_indicators\", {}),\n",
        "            \"column_stats\": {}\n",
        "        }\n",
        "\n",
        "        # Calculate relevant statistics\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"mean\": float(df[col].mean()),\n",
        "                    \"std\": float(df[col].std()),\n",
        "                    \"min\": float(df[col].min()),\n",
        "                    \"max\": float(df[col].max())\n",
        "                }\n",
        "            elif df[col].dtype == 'bool':\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"true_count\": int(df[col].sum()),\n",
        "                    \"false_count\": int((~df[col]).sum()),\n",
        "                    \"true_percentage\": float(df[col].mean() * 100)\n",
        "                }\n",
        "            else:\n",
        "                analysis_data[\"column_stats\"][col] = {\n",
        "                    \"unique_count\": int(df[col].nunique()),\n",
        "                    \"top_values\": df[col].value_counts().head(5).to_dict()\n",
        "                }\n",
        "\n",
        "        prompt = f\"\"\"You are analyzing a {self.context['dataset_type']} dataset to find problems.\n",
        "\n",
        "DATASET CONTEXT:\n",
        "{json.dumps(self.context, indent=2)}\n",
        "\n",
        "STATISTICAL ANALYSIS:\n",
        "{json.dumps(analysis_data, indent=2, default=str)}\n",
        "\n",
        "SAMPLE RECORDS:\n",
        "{json.dumps(df.head(10).to_dict('records'), indent=2, default=str)}\n",
        "\n",
        "TASK: Identify specific, actionable problems that require autonomous intervention.\n",
        "\n",
        "Return ONLY a JSON object:\n",
        "{{\n",
        "  \"problems\": [\n",
        "    {{\n",
        "      \"problem_id\": \"PROB-001\",\n",
        "      \"severity\": \"CRITICAL|HIGH|MEDIUM|LOW\",\n",
        "      \"category\": \"category name\",\n",
        "      \"description\": \"specific problem description\",\n",
        "      \"affected_records\": \"how many/which records\",\n",
        "      \"business_impact\": \"what's the business impact\",\n",
        "      \"requires_action\": true/false,\n",
        "      \"suggested_actions\": [\"action1\", \"action2\"]\n",
        "    }}\n",
        "  ],\n",
        "  \"summary\": {{\n",
        "    \"total_problems\": 0,\n",
        "    \"critical_count\": 0,\n",
        "    \"high_count\": 0,\n",
        "    \"requires_immediate_action\": true/false\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Focus on REAL problems in the data, not hypothetical ones.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        result = clean_json_response(response_text)\n",
        "\n",
        "        self.problems_found = result.get(\"problems\", [])\n",
        "        self.problem_analysis_report = result\n",
        "\n",
        "        # Store the problem analysis in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['problem_analysis_report'] = self.problem_analysis_report\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored problem analysis in session {self.session_id}.\")\n",
        "\n",
        "        # Store the problem analysis in long-term memory\n",
        "        memory_key = f\"problem_analysis_{self.session_id}\"\n",
        "        self.memory_service.store_information(key=memory_key, value=self.problem_analysis_report, metadata={'session_id': self.session_id, 'timestamp': utc_now()})\n",
        "        print(f\"Stored problem analysis in MemoryBank with key: {memory_key}.\")\n",
        "\n",
        "        print(f\"\\n‚ö†Ô∏è  PROBLEM SCAN COMPLETE\")\n",
        "        print(f\"   Total Problems Found: {result['summary']['total_problems']}\")\n",
        "        print(f\"   Critical: {result['summary']['critical_count']}\")\n",
        "        print(f\"   High: {result['summary']['high_count']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# Re-instantiate and run problem detection with session and memory service\n",
        "problem_detector = ProblemDetectionAgent(dataset_context, session_service, session_id, memory_service)\n",
        "problem_analysis = problem_detector.scan_dataset(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved problem analysis from session analysis_session_20251129_151936.\n",
            "Using cached problem analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4330d00"
      },
      "source": [
        "## Integrate Long-Term Memory into AutonomousPlannerAgent\n",
        "\n",
        "### Subtask:\n",
        "Integrate MemoryBank into AutonomousPlannerAgent to store generated plans.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d03aafa",
        "outputId": "82551cdb-c70e-4fa6-82be-a17387929448"
      },
      "source": [
        "session_data = session_service.get_session(session_id)\n",
        "if 'execution_plans' in session_data:\n",
        "    del session_data['execution_plans']\n",
        "    session_service.update_session(session_id, session_data)\n",
        "    print(f\"Cleared 'execution_plans' from session {session_id} to force re-planning.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Cleared 'execution_plans' from session analysis_session_20251129_151936 to force re-planning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d5308344",
        "outputId": "afd546f5-eaa2-48a0-8ae3-0c9df7fc010c"
      },
      "source": [
        "class AutonomousPlannerAgent:\n",
        "    \"\"\"Creates detailed action plans for each problem\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict, session_service: SessionService, session_id: str, memory_service: MemoryService):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "        self.memory_service = memory_service # Store memory service\n",
        "        self.plans = []\n",
        "\n",
        "        # Attempt to retrieve previous plans from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'execution_plans' in session_data:\n",
        "            self.plans = session_data['execution_plans']\n",
        "            print(f\"Retrieved {len(self.plans)} plans from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous plans found in session {self.session_id}.\")\n",
        "\n",
        "    def create_plan(self, problem: Dict, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate detailed autonomous action plan\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"You are an autonomous planning agent for {self.context['dataset_type']}.\n",
        "\n",
        "PROBLEM TO SOLVE:\n",
        "{json.dumps(problem, indent=2)}\n",
        "\n",
        "BUSINESS CONTEXT:\n",
        "- Domain: {self.context['domain']}\n",
        "- Key entities: {self.context['key_entities']}\n",
        "- Available actions: {self.context['autonomous_actions']}\n",
        "\n",
        "TASK: Create a detailed, step-by-step execution plan.\n",
        "\n",
        "Return ONLY a JSON object:\n",
        "{{\n",
        "  \"plan_id\": \"PLAN-{problem['problem_id']}\",\n",
        "  \"problem_reference\": \"{problem['problem_id']}\",\n",
        "  \"objective\": \"clear statement of what we're trying to achieve\",\n",
        "  \"execution_steps\": [\n",
        "    {{\n",
        "      \"step_number\": 1,\n",
        "      \"action_type\": \"API_CALL|DATABASE_QUERY|NOTIFICATION|REROUTE|REORDER\",\n",
        "      \"description\": \"what to do\",\n",
        "      \"tool_needed\": \"specific tool/API\",\n",
        "      \"parameters\": {{}},\n",
        "      \"expected_outcome\": \"what success looks like\",\n",
        "      \"rollback_plan\": \"what to do if this fails\"\n",
        "    }}\n",
        "  ],\n",
        "  \"success_criteria\": [\"criterion1\", \"criterion2\"],\n",
        "  \"estimated_impact\": \"quantifiable benefit\",\n",
        "  \"estimated_time\": \"how long to execute\",\n",
        "  \"dependencies\": [\"what needs to happen first\"]\n",
        "}}\n",
        "\n",
        "Make the plan realistic and executable with specific details.\"\"\"\n",
        "\n",
        "        response_text = safe_api_call(self.model, prompt)\n",
        "        plan = clean_json_response(response_text)\n",
        "\n",
        "        plan[\"created_at\"] = utc_now()\n",
        "        self.plans.append(plan)\n",
        "\n",
        "        # Store the updated plans list in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['execution_plans'] = self.plans\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored updated plans in session {self.session_id}.\")\n",
        "\n",
        "        # Store the plan in long-term memory\n",
        "        memory_key = f\"plan_{plan['plan_id']}_{self.session_id}\"\n",
        "        self.memory_service.store_information(key=memory_key, value=plan, metadata={'session_id': self.session_id, 'problem_id': plan['problem_reference'], 'timestamp': utc_now()})\n",
        "        print(f\"Stored plan '{plan['plan_id']}' in MemoryBank with key: {memory_key}.\")\n",
        "\n",
        "        return plan\n",
        "\n",
        "# Create plans for each problem with session and memory service\n",
        "planner_agent = AutonomousPlannerAgent(dataset_context, session_service, session_id, memory_service)\n",
        "execution_plans = []\n",
        "\n",
        "print(\"\\n\\u2501\"*60)\n",
        "print(\"\\u2501 CREATING AUTONOMOUS PLANS...\")\n",
        "print(\"\\u2501\"*60)\n",
        "for i, problem in enumerate(problem_detector.problems_found):\n",
        "    if problem.get(\"requires_action\"):\n",
        "        print(f\"\\n[{i+1}] Planning for: {problem['problem_id']}\")\n",
        "        plan = planner_agent.create_plan(problem, df)\n",
        "        execution_plans.append(plan)\n",
        "        print(f\"   \\u2713 Plan created: {len(plan['execution_steps'])} steps\")\n",
        "        time.sleep(1)  # Rate limiting"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous plans found in session analysis_session_20251129_151936.\n",
            "\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ\n",
            "‚îÅ CREATING AUTONOMOUS PLANS...\n",
            "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
            "\n",
            "[1] Planning for: PROB-001\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated plans in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'plan_PLAN-PROB-001_analysis_session_20251129_151936'.\n",
            "Stored plan 'PLAN-PROB-001' in MemoryBank with key: plan_PLAN-PROB-001_analysis_session_20251129_151936.\n",
            "   ‚úì Plan created: 8 steps\n",
            "\n",
            "[2] Planning for: PROB-002\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated plans in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'plan_PLAN-PROB-002_analysis_session_20251129_151936'.\n",
            "Stored plan 'PLAN-PROB-002' in MemoryBank with key: plan_PLAN-PROB-002_analysis_session_20251129_151936.\n",
            "   ‚úì Plan created: 7 steps\n",
            "\n",
            "[3] Planning for: PROB-003\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated plans in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'plan_PLAN-PROB-003_analysis_session_20251129_151936'.\n",
            "Stored plan 'PLAN-PROB-003' in MemoryBank with key: plan_PLAN-PROB-003_analysis_session_20251129_151936.\n",
            "   ‚úì Plan created: 6 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "033f5885"
      },
      "source": [
        "## Integrate Long-Term Memory into ExecutionAgent\n",
        "\n",
        "### Subtask:\n",
        "Integrate MemoryBank into ExecutionAgent to store execution logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3e4dbe9d",
        "outputId": "7f287ab9-7358-409b-ea18-669fdcd9f5fd"
      },
      "source": [
        "class ExecutionAgent:\n",
        "    \"\"\"Simulates execution of autonomous plans\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_context: Dict, session_service: SessionService, session_id: str, memory_service: MemoryService):\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.context = dataset_context\n",
        "        self.session_service = session_service\n",
        "        self.session_id = session_id\n",
        "        self.memory_service = memory_service # Store memory service\n",
        "        self.execution_logs = []\n",
        "\n",
        "        # Attempt to retrieve previous execution results from session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        if 'execution_results' in session_data:\n",
        "            self.execution_logs = session_data['execution_results']\n",
        "            print(f\"Retrieved {len(self.execution_logs)} execution logs from session {self.session_id}.\")\n",
        "        else:\n",
        "            print(f\"No previous execution logs found in session {self.session_id}.\")\n",
        "\n",
        "    def execute_plan(self, plan: Dict) -> Dict:\n",
        "        \"\"\"Execute each step of the plan\"\"\"\n",
        "\n",
        "        print(f\"\\n‚Ñõ EXECUTING: {plan['plan_id']}\")\n",
        "        print(f\"   Objective: {plan['objective']}\")\n",
        "\n",
        "        execution_log = {\n",
        "            \"plan_id\": plan['plan_id'],\n",
        "            \"started_at\": utc_now(),\n",
        "            \"steps_executed\": [],\n",
        "            \"status\": \"IN_PROGRESS\"\n",
        "        }\n",
        "\n",
        "        for i, step in enumerate(plan['execution_steps']):\n",
        "            step_number = step.get('step_number', i + 1) # Use existing step_number or assign based on index\n",
        "            print(f\"\\n   Step {step_number}: {step['description']}\")\n",
        "\n",
        "            # Simulate execution with AI\n",
        "            prompt = f\"\"\"Simulate executing this autonomous action:\\n\\nSTEP: {json.dumps(step, indent=2)}\\n\\nCONTEXT: {self.context['dataset_type']} system\\n\\nSimulate realistic execution and return result in JSON:\\n{{\\n  \"step_number\": {step_number},\\n  \"status\": \"SUCCESS|FAILED|PARTIAL\",\\n  \"action_taken\": \"specific action performed\",\\n  \"result_data\": {{}},\\n  \"metrics\": {{}},\\n  \"notes\": \"important details\",\\n  \"timestamp\": \"{utc_now()}\"\\n}}\\n\\nBe realistic about what would actually happen.\"\"\"\n",
        "\n",
        "            response_text = safe_api_call(self.model, prompt)\n",
        "            result = clean_json_response(response_text)\n",
        "\n",
        "            execution_log[\"steps_executed\"].append(result)\n",
        "\n",
        "            status_icon = \"‚úì\" if result[\"status\"] == \"SUCCESS\" else \"‚úó\"\n",
        "            print(f\"      {status_icon} {result['status']}: {result['action_taken']}\")\n",
        "\n",
        "            time.sleep(1)  # Rate limiting\n",
        "\n",
        "        execution_log[\"completed_at\"] = utc_now()\n",
        "        execution_log[\"status\"] = \"COMPLETED\" if all(\n",
        "            s[\"status\"] == \"SUCCESS\" for s in execution_log[\"steps_executed\"]\n",
        "        ) else \"PARTIAL\"\n",
        "\n",
        "        self.execution_logs.append(execution_log)\n",
        "\n",
        "        # Store the updated execution logs list in the session\n",
        "        session_data = self.session_service.get_session(self.session_id)\n",
        "        session_data['execution_results'] = self.execution_logs\n",
        "        self.session_service.update_session(self.session_id, session_data)\n",
        "        print(f\"Stored updated execution logs in session {self.session_id}.\")\n",
        "\n",
        "        # Store the execution log in long-term memory\n",
        "        memory_key = f\"execution_log_{plan['plan_id']}_{self.session_id}\"\n",
        "        self.memory_service.store_information(key=memory_key, value=execution_log, metadata={'session_id': self.session_id, 'plan_id': plan['plan_id'], 'timestamp': utc_now()})\n",
        "        print(f\"Stored execution log for plan '{plan['plan_id']}' in MemoryBank with key: {memory_key}.\")\n",
        "\n",
        "        print(f\"\\n   ‚úÖ Plan {execution_log['status']}\")\n",
        "\n",
        "        return execution_log\n",
        "\n",
        "# Clear previous execution results from the session to force re-execution and re-storage\n",
        "session_data = session_service.get_session(session_id)\n",
        "if 'execution_results' in session_data:\n",
        "    del session_data['execution_results']\n",
        "    session_service.update_session(session_id, session_data)\n",
        "    print(f\"Cleared 'execution_results' from session {session_id} to force re-execution.\")\n",
        "\n",
        "\n",
        "# Execute all plans with session and memory service\n",
        "executor_agent = ExecutionAgent(dataset_context, session_service, session_id, memory_service)\n",
        "execution_results = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ AUTONOMOUS EXECUTION STARTED\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for plan in planner_agent.plans: # Use plans from planner_agent which are already in session\n",
        "    result = executor_agent.execute_plan(plan)\n",
        "    execution_results.append(result)\n",
        "    time.sleep(2)  # Rate limiting between plans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Cleared 'execution_results' from session analysis_session_20251129_151936 to force re-execution.\n",
            "No previous execution logs found in session analysis_session_20251129_151936.\n",
            "\n",
            "============================================================\n",
            "üöÄ AUTONOMOUS EXECUTION STARTED\n",
            "============================================================\n",
            "\n",
            "‚Ñõ EXECUTING: PLAN-PROB-001\n",
            "   Objective: To identify and address the root causes of delivery delays and improve the overall on-time delivery rate.\n",
            "\n",
            "   Step 1: Analyze on-time delivery percentage aggregated by 'origin_warehouse' to identify warehouses with a disproportionately high number of late deliveries.\n",
            "      ‚úì SUCCESS: Executed SQL query to analyze on-time delivery percentage by origin_warehouse.\n",
            "\n",
            "   Step 2: Analyze on-time delivery percentage aggregated by 'destination_store' to identify stores that are frequently receiving late deliveries.\n",
            "      ‚úì SUCCESS: Executed SQL query against the shipment_delivery_performance database.\n",
            "\n",
            "   Step 3: Analyze on-time delivery percentage aggregated by 'product_id' to identify specific products that are more prone to delays.\n",
            "      ‚úì SUCCESS: Executed SQL query to analyze on-time delivery percentage by product_id. The query joined 'shipments' and 'products' tables, calculated total shipments, late shipments, and the percentage of late shipments for each product, then ordered the results in descending order of late percentage.\n",
            "\n",
            "   Step 4: Analyze on-time delivery percentage aggregated by 'driver_identifier' to identify drivers with consistently poor on-time performance.\n",
            "      ‚úì SUCCESS: Executed SQL query to analyze on-time delivery percentage by driver.\n",
            "\n",
            "   Step 5: Notify the logistics manager and relevant warehouse/store managers about the identified high-incidence origins and destinations for delivery delays.\n",
            "      ‚úì SUCCESS: Sent email notification to specified recipients with the provided subject and body, replacing bracketed placeholders with simulated data.\n",
            "\n",
            "   Step 6: Flag drivers with a significantly high percentage of late deliveries (e.g., > 20% lateness) for review and potential retraining.\n",
            "      ‚úì SUCCESS: Sent an email notification to the driver management team regarding drivers with a high percentage of late deliveries.\n",
            "\n",
            "   Step 7: Initiate analysis of product delivery patterns for high-risk products identified in Step 3 to understand if specific product characteristics or quantities are contributing to delays.\n",
            "      ‚úì SUCCESS: Called product_delivery_analysis_api with parameters: {\"product_ids\": [\"PROD_XYZ789\", \"PROD_ABC123\", \"PROD_LMN456\"], \"analysis_type\": \"root_cause_delays\"}\n",
            "\n",
            "   Step 8: For identified delayed shipments, use the re-routing API to suggest alternative delivery methods or routes to minimize further delays.\n",
            "      ‚úì SUCCESS: Called shipment_management_api.recommend_reroute_or_alternative_method with shipment_ids=['SHIP12345', 'SHIP67890', 'SHIPABCDE']\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated execution logs in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'execution_log_PLAN-PROB-001_analysis_session_20251129_151936'.\n",
            "Stored execution log for plan 'PLAN-PROB-001' in MemoryBank with key: execution_log_PLAN-PROB-001_analysis_session_20251129_151936.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n",
            "\n",
            "‚Ñõ EXECUTING: PLAN-PROB-002\n",
            "   Objective: Determine the on-time delivery rate for the product 'capes' and identify if other high-frequency products exhibit similar late delivery patterns to proactively address potential systemic issues.\n",
            "\n",
            "   Step 1: Query the shipment data to retrieve all deliveries associated with the product 'capes'.\n",
            "      ‚úì SUCCESS: Executed SQL query: SELECT * FROM shipments WHERE product_id = (SELECT product_id FROM products WHERE name = 'capes');\n",
            "\n",
            "   Step 2: Calculate the on-time delivery rate for 'capes' based on the retrieved shipment data. This involves comparing the actual delivery date with the scheduled delivery date for each shipment.\n",
            "      ‚úì SUCCESS: Executed Python script using Pandas to calculate the on-time delivery rate for 'capes'. The script filtered the 'output_from_step_1' data to include only shipments of 'capes', then compared the 'actual_delivery_date' with the 'scheduled_delivery_date' to count on-time deliveries. The on-time delivery rate was calculated as (on_time_count / total_shipments) * 100.\n",
            "\n",
            "   Step 3: Identify the top 10 most frequent products (by number of deliveries) from the shipment data.\n",
            "      ‚úì SUCCESS: Executed SQL query to retrieve the top 10 most frequent products from the shipments table.\n",
            "\n",
            "   Step 4: For each of the top 10 high-frequency products identified in Step 3, query their respective on-time delivery rates.\n",
            "      ‚úì SUCCESS: Executed SQL queries to retrieve on-time delivery rates for the top 10 high-frequency products. The queries iterated through a list of product IDs obtained from Step 3. For each product_id, the following query was executed: SELECT AVG(CASE WHEN actual_delivery_date <= scheduled_delivery_date THEN 1 ELSE 0 END) * 100 FROM shipments WHERE product_id = '{product_id}';. The results were aggregated into a single dataset.\n",
            "\n",
            "   Step 5: Compare the on-time delivery rate of 'capes' with the on-time delivery rates of other high-frequency products. Flag products with on-time delivery rates significantly lower than the average or below a defined threshold (e.g., 90%).\n",
            "      ‚úì SUCCESS: Executed Python script to compare on-time delivery rates. The script calculated the average on-time delivery rate for other high-frequency products, compared 'capes' on-time delivery rate to this average and a fixed threshold of 90%. Products falling below the threshold or significantly lower than the average were identified.\n",
            "\n",
            "   Step 6: Utilize the 'Analyze product delivery patterns to identify high-risk products or routes' action for products flagged in Step 5, focusing on 'capes' initially.\n",
            "      ‚úì SUCCESS: Called 'Analyze product delivery patterns' tool from Internal Analysis Module.\n",
            "\n",
            "   Step 7: Notify relevant stakeholders (e.g., product managers, logistics managers) about the on-time delivery performance of 'capes' and any other identified high-risk products. Include the calculated on-time rates and potential reasons identified in Step 6.\n",
            "      ‚úì SUCCESS: Sent an email notification to product_managers@example.com and logistics_managers@example.com.\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated execution logs in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'execution_log_PLAN-PROB-002_analysis_session_20251129_151936'.\n",
            "Stored execution log for plan 'PLAN-PROB-002' in MemoryBank with key: execution_log_PLAN-PROB-002_analysis_session_20251129_151936.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n",
            "\n",
            "‚Ñõ EXECUTING: PLAN-PROB-003\n",
            "   Objective: Investigate and mitigate quantity variances in product shipments to understand root causes and prevent future discrepancies.\n",
            "\n",
            "   Step 1: Retrieve historical shipment data, including 'product_quantity', 'product_id', 'origin_warehouse_id', and 'destination_store_id'. Also, retrieve any available 'expected_quantity' data if it exists in a related table (e.g., orders table).\n",
            "      ‚úì SUCCESS: Executed SQL query against the shipment_delivery_performance database.\n",
            "\n",
            "   Step 2: Compare 'product_quantity' with 'expected_quantity' (if available) for each shipment. Calculate the variance and flag shipments with significant deviations. If 'expected_quantity' is not available, analyze the distribution of 'product_quantity' by 'product_id', 'origin_warehouse_id', and 'destination_store_id' to identify outliers and patterns.\n",
            "      ‚úì SUCCESS: Executed 'analyze_quantity_variance.py' using the Pandas library. The script successfully compared 'product_quantity' with 'expected_quantity' where available, calculated variances, and flagged shipments exceeding a predefined significance threshold (e.g., 10% deviation). For shipments lacking 'expected_quantity', it performed outlier analysis on 'product_quantity' grouped by 'product_id', 'origin_warehouse_id', and 'destination_store_id'.\n",
            "\n",
            "   Step 3: Retrieve 'on_time' status for shipments identified in the previous step.\n",
            "      ‚úì SUCCESS: Executed SQL query 'SELECT shipment_id, on_time FROM shipments WHERE shipment_id IN (:identified_shipment_ids);' using the Database Query Tool. The placeholder ':identified_shipment_ids' was dynamically replaced with the shipment IDs retrieved in the previous step.\n",
            "\n",
            "   Step 4: Analyze if the identified quantity discrepancies correlate with the 'on_time' status. For example, are shipments with quantity overages more likely to be late or early?\n",
            "      ‚úì SUCCESS: Executed Python script 'correlate_variance_on_time.py' using Pandas. The script merged the dataframes generated in Step 2 (quantity discrepancies) and Step 3 (on_time status). It then performed a correlation analysis between the 'quantity_variance_percentage' and 'on_time_status' columns. A statistical test (e.g., chi-squared for categorical 'on_time_status' against continuous 'quantity_variance_percentage', or point-biserial correlation if 'on_time_status' is dichotomized) was applied. The script generated a report summarizing the findings, including correlation coefficients, p-values, and visualizations (e.g., scatter plot with trend line, box plots of variance by on-time status).\n",
            "\n",
            "   Step 5: Generate a summary report detailing the findings: identification of specific products, warehouses, or stores with quantity variance issues, and any observed correlations with on-time performance. This report should be sent to the logistics management team and relevant operations personnel.\n",
            "      ‚úì SUCCESS: Sent an email notification to 'logistics_management@example.com' and 'operations_team@example.com' with the subject 'Shipment Quantity Variance Analysis - PROB-003' and the body 'Detailed report on quantity variance findings and correlations with on-time performance. Please review for corrective actions.' The report content, generated in the previous step (not detailed here), would have been embedded or attached.\n",
            "\n",
            "   Step 6: Trigger an alert to be used in the future: 'Alert when product quantities significantly deviate from expected values for a given shipment.' This action will leverage the insights gained to proactively monitor for future occurrences.\n",
            "      ‚úì SUCCESS: Configured a new alert rule in the Monitoring and Alerting System.\n",
            "Session 'analysis_session_20251129_151936' updated.\n",
            "Stored updated execution logs in session analysis_session_20251129_151936.\n",
            "Information stored for key: 'execution_log_PLAN-PROB-003_analysis_session_20251129_151936'.\n",
            "Stored execution log for plan 'PLAN-PROB-003' in MemoryBank with key: execution_log_PLAN-PROB-003_analysis_session_20251129_151936.\n",
            "\n",
            "   ‚úÖ Plan COMPLETED\n"
          ]
        }
      ]
    }
  ]
}